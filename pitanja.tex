% !TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}

\usepackage{float}
\restylefloat{table}
\usepackage[utf8x,utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[inline]{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\urlstyle{same}

\begin{document}

\title{Istrazivanje Podataka}

\author{Daniel Do≈æa}

\maketitle

\section{Sta je istrazivanje podataka?}

Ne postoji precizna definicija istrazivanja podataka, ali se smatra da je to skup algoritama i
tehnika koji omogucava automatsko zakljucivanje nekih cinjenica i veza iz velikog skupa podataka,
koji bi mozda inace ostali neotkriveni.

\section{Koji su zadaci istrazivanja podataka?}

Zadaci u istrazivanju podataka se mogu podeliti u dve grupe: zadaci opisivanja i zadaci
predvidjanja. Zadaci predvidjanja bave se odredjivanjem vrednosti nekog atributa na osnovu vrednosti
neki drugih atributa. Ove atribute nazivamo \emph{zavisne promenljive} ili \emph{ciljne promenljive}
a atribute na osnovu kojih dolazimo do zakljucaka \emph{opisne promenljive} ili \emph{nezavisne
promenljive}.

\begin{enumerate}
    \item \textbf{Klasifikacija} pravimo funkciju od nezavisnih varijabli da bi dobili zavisne
        varijable
    \begin{itemize}
        \item klasifikacija - podatke, na primer: da li ce kupac kupiti neki artikal
        \item regresija - za kontinualne podatke, na primer: koliko ce stepeni biti u neko vreme
    \end{itemize}
    \item \textbf{Prepoznavanje obrazaca} - nalazimo veze u podacima: na primer ako je kupac kupio
        pelene, ver
    \item \textbf{Klasterovanje} - grupisemo podatke prema slicnosti, na primer: grupisemo dokumente
        vezane za ekonomiju u jedan klaster, a dokumente vezane za medicinu u drugi klaster.
    \item \textbf{Otkrivanje anomalija} - otkrivanje podataka koji su veoma drugaciji od ostalih.
        Ove podatke nazivamo anomalije ili autlajeri
\end{enumerate}

\section{Sta su podaci?}

Skup podataka mozemo predstaviti kao skup \emph{objekata}. Ovi objekti se takodje mogu nazivati i
\emph{slog}, \emph{slucaj}, \emph{uzorak}, \emph{vektor}\ldots Objekte opisuju njihovi
\emph{atributi} koji nam govore o osobinama tog objekta, na primer: boja, visina, tezina\ldots
Atributi se jos nazivaju i \emph{karakteristika}, \emph{varijabla}, \emph{polje}\ldots

\section{Sta je atribut?}

Atribut je karakteristika objekta. Atribut moze da varira izmedju razlicitih objekata ili da varira
u vremenu. Boja ociju je razlicita za dva coveka, a na primer, temperatura nekog predmeta varira u
toku vremena.

\section{Sta je vrednost atributa?}

Vrednost atributa je broj ili simbol koji je pridruzen atributu. Moramo razlikovati vrednost
atributa od samog atributa, jer vrednosti mogu da imaju neke osobine koje atribut nema, i obrnuto.
Na primer ako za zaposlenog cuvamo identifikacioni broj i broj godina, ima smisla racunati prosek
godina, dok nema smisla racunati prosek identifikacionog broja. Jedina logicna operacija sa
identifikacionim brojevima je poredjenje jednakosti.

\section{Kako odredjujemo tip atributa?}

Tip atributa mozemo odrediti na osnovu broja vrednosti koji moze da sadrzi:

\begin{itemize}
    \item \textbf{diskretni} (konacni, ili prebrojivo beskonacni skupovi vrednosti, primer:
        postanski brojevi)
    \item \textbf{neprekidni} (realni brojevi, primer: temperatura, tezina, pritisak, brzina)
\end{itemize}

\emph{Asimetricni atributi} - kod njih je bitna samo ne-nula vrednost. Na
primer vrednost 1 ako je student pohadjao kurs, a vrednost 0 ako nije. U tom slucaju bi nas zanimali
samo studenti sa vrednoscu 1.

Takodje, tip atributa mozemo posmatrati na osnovu operacija koje se mogu izvrsiti nad njihovim
vrednostima

\begin{center}\begin{tabular}{llll}
    \hline
    Vrsta Operacije & Rbr & Operacija & Tip Atributa \\ \hline
    razlicitost & 1 & \(=\) i \(\neq\)& Imenski(1) \\
    uredjenje & 2 & \(<\), \(>\), \(\leqslant\), \(\geqslant\) & Redni(1,2) \\
    aditivnost & 3 & \(+\), \(-\) & Intervalni(1,2,3) \\
    multiplikativnost & 4 & \(\times\), \(\div\) & Razmerni(1,2,3,4) \\ \hline
\end{tabular}\end{center} 

\begin{itemize}
    \item \textbf{Kategoricki} - imenski i redni
    \item \textbf{Numericki} - intervalni i razmerni
\end{itemize}

\section{Koje su karakteristike skupa podataka?}

\begin{enumerate}
    \item dimenzionalnost - predstavlja broj atributa koje objekti imaju
    \item retkost - uzmimo na primer asimetricne podatke, mozda razmatramo samo 1\% od ukupnog broja
        objekata
    \item rezolucija - sa kojim nivoom detalja gledamo na podatke. Na primer zemlja gledana sa par
        metara je proprilicno neravna, a gledano sa par desetina kilometara je poprilicno glatka.
\end{enumerate}

\section{Koji su tipovi skupa podataka?}

Tipovi podataka nisu precizno definisani, ali ih mozemo grupisati u tri grube kategorije:

\begin{enumerate}
    \item Slogovi
    \item Grafovski podaci
    \item Uredjeni podaci
\end{enumerate}

\section{Sta su slogovi?}

Podrazumeva da su podaci organizovani kao torke koje predstavljaju objekte. Svaki objekat ima fiksan
broj atributa. U svojoj osnovnoj formi, smatramo da ne postoje veze izmedju torki, kao ni izmedju
atributa, i svaki objekat ima isti skup atributa.

\paragraph{Transakcije}

Transakcija je specijalan slucaj torki. Asocijacija se moze napraviti sa \emph{potrosackom korpom}.
Svaka torka sadrzi skup elemenata koje je kupac kupio u jednoj kupovini.

\paragraph{Matrice podataka}

Ukoliko svi objekti imaju isti broj atributa, podaci se mogu posmatrati kao matrica. Redovi ove
matrice predstavljaju objekte, a kolone predstavljaju atribute (obrnuto je takodje dozvoljeno). U
ovakvom uredjenju, objekte mozemo posmatrati kao \(n\)-dimenizione vektore, gde su
dimenzije odredjene atributima.

\section{Sta su grafovski podaci?}

\paragraph{Grafovi sa vezama izmedju podataka u granama}

Objekti su u ovom modeli predstavljeni kao cvorovi grafa, dok su veze izmedju objekata prikazane
granama. Kao primer mozemo uzeti Web stranice.

\paragraph{Objekti predstavljeni kao grafovi}

Ako su objekti sacinjeni od podobjekata koji imaju medjusobne veze, takve objekte cesto
predstavljamo kao grafove. Kao primer mozemo uzeti hemijska jedinjenja, gde cvorovi predstavljaju
atome, a grane predstavljaju hemijske veze.

\section{Sa su uredjeni podaci?}

Ponekad su podaci uredjeni na osnovu vremena i/ili prostora.

\paragraph{Vremenski podaci}

Uredjeni su kao slogovi, s tim sto je svakom slogu pridruzeno vreme. Ovim mozemo odredjivati na
primer porast u kupovini slatkisa pred noc vestica.

\paragraph{Podaci u odredjenom redosledu}

Slicni su vremenskim podacima, ali nemaju vremenske odrednice, vec su poredjani u uredjenom
rasporedu. Primer su genetske sekvence.

\paragraph{Serijski podaci}

Vrlo slicni vremenskim podacima, samo sto svaki objekat cini serija podataka izmerenih u toku nekog
perioda.

\paragraph{Prostorni podaci}

Objekti sadrze prostorne odrednice. Primer bi bio podaci o vremenskoj prognozi.

\section{Sta je prepoznavanje obrazaca?}

Prepoznavanje obrazaca se moze, u osnovnoj formi predstaviti preko binarne matrice. Moze se
posmatrati takva matrica da kolone predstavljaju artikle, a redovi transakcije kupaca. Polje
\((i,j)\) je jednako 1 ako je kupac kupio artikal, inace je 0. Cilj nam je da prepoznamo da li postoje
neka pravila u kupovini, tojest, da li su vece sanse da kupac kupi stvar \(x\) ako je kupio stvar
\(y\). Formalno: neka je data binarna matrica \(n \times d\), posmatramo podskupove kolona, takve
da sve imaju vrednost 1. Svakom tom podskupu se dodeljuje podrska \(s\), koja predstavlja
ucestalost ponavljanja tog podskupa u odnosu na ceo skup. Ukoliko je \(s\) vece od minimalne
podrske, smatramo da je obrazac cest.

\section{Sta je podrska pravila pridruzivanja?}

Neka su \(A\) i \(B\) dva skupa obrazaca. Podrska \(sup(A \implies B)\) je
definfinisana kao \(\#(A \cup B) / N\), Gde \(\#(A \cup B)\) predstavlja broj
zadovoljavajucih obrazaca, a \(N\) ukupan broj redova u kompletnom skupu.

\section{Sta je pouzdanost pravila pridruzivanja?}

Neka su A i B dva skupa obrazaca. Pouzdanost \(conf(A \implies B)\) je definisana kao
\(\#(A \cup B) / \#(A)\).

\section{Sta je klasterovanje? (gruba definicija)}

Klasterovanje je grupisanje objekata na osnovu njihove `slicnosti'. Uvidja se da je od velike
vaznosti dizajn funkcije `slicnosti'.

\section{Sta je otkrivanje elemenata van granica? (gruba definicija)}

Zadatak je otkriti element koji je u velikoj meri razlicit od svih ostalih. Ovaj element zovemo
anomalija ili autlajer. Mozemo autlajere posmatrati i kao `element koji je toliko razlicit od
ostalih, da se moze posumnjati da je nastao nekim razlicitim mehanizmom'. Primeri bi bili upad u
racunarski sistem, zloupotreba kreditnih kartica\ldots

\section{Sta je klasifikacija? (gruba definicija)}

Klasifikacija je problem odredjivanja vrednosti nekog specijalnog atributa. Problem klasifikacije je
problem nadgledanog ucenja. Formiramo skup podataka koje nazivamo  podaci za trening, na osnovu
kojih nas algoritam odredjuje odnos ostalih atributa i specijalnog atributa koji se trazi. Nakon
toga test podaci se koriste da se utvrdi preciznost algoritma i eventualno podese parametri u cilju
povecanja preciznosti. Onda mozemo koristiti dobijen algoritam za odredjivanje specijalnog atributa
u skupovima podataka gde je on nepoznat.

\section{Navesti par primera istrazivanja podataka}

\begin{enumerate}
    \item rasporedjivanje artikala u prodavnici
    \item prepouke kupcima
    \item anomalije u logovima aplikacija
\end{enumerate}

\section{Sta je slicnost/razlicitost objekata, obrazaca i atributa?}

Slicnost i razlicitost izmedju objekata su funkcije koje nam govore o tome koliko su dva objekta
slicna ili razlicita. Vece vrednosti funkcije slicnosti nam govore da su objekti vise slicni, a
obrnuto vazi za funkcije razlicitosti.  Uglavnom se funkcije slicnosti mere u vrednostima na
intervalu [0, 1], dok se funkcije razlicitosti mere na intervalu od 0 (objekti su isti) na vise.
Koriste se i termini rastojanje (distance), blizina (proximity).

Funkcije slicnosti i razlicitosti su od velikog znacaja, jer imaju uticaj na svaki problem u
istrazivanju podataka. Los izbor funkcije slicnosti moze da ima presudnu vrednost u tome da li smo
odradili dobar posao. Ova cinjenica nam govori da ne smemo zapostaviti izbor funkcije slicnosti i
samo se fokusirati na algoritamski deo problema istrazivanja podataka.

\section{Navesti primer funkcije slicnosti/razlicitosti za nominalne atribute \(p\) i \(q\).}

\begin{itemize}
\item \textbf{Slicnost}

\[sim(p, q) = 1 \iff p = q\]
\[sim(p, q) = 0 \iff p \neq q\]

\item \textbf{Razlicitost}

\[dist(p, q) = 0 \iff p = q\]
\[dist(p, q) = 1 \iff p \neq q\]

\end{itemize}

\section{Navesti primer funkcije slicnosti/razlicitosti za redne atribute \(p\) i \(q\)}.

Ako \(p\) i \(q\) mogu imati \(n\) razlicitih vrednosti, onda funkcije slicnosti i
razlicitosti definisemo na sledeci nacin:

\begin{itemize}
\item \textbf{Slicnost}

\[sim(p, q) = 1 - \dfrac{|p - q|}{n - 1}\]

\item \textbf{Razlicitost}

\[dist(p, q) = \dfrac{|p - q|}{n - 1}\]
\end{itemize}

\section{Navesti primer funkcije slicnosti/razlicitosti za intervalne i razmerne atribute \(p\) i
\(q\).}

\begin{itemize}
\item \textbf{Slicnost}

\[sim(p, q) = -dist(p,q)\]
\[sim(p, q) = \dfrac{1}{1 + dist(p, q)}\]

\item \textbf{Razlicitost}

\[dist(p, q) = |p - q|\]
\end{itemize}

\section{Sta treba da vazi za funkciju rastojanja \(d\) da bi ona bila metrika?}

Da bi funkcija rastojanja \(d\) je metrika ako i samo ako vazi:

\begin{enumerate}
    \item Pozitivna odredjenost
        \[d(p, q) \geq 0, \forall p, q\]
        \[d(p, q) = 0, \iff p = q\]
    \item Simetrija
        \[d(p, q) = d(q, p)\]
    \item Nejednakost trougla
        \[d(p, q) \leq d(p, r) + d(r, q)\]
\end{enumerate}

\section{Sta treba da vazi za funkciju rastojanja \(d\) da bi ona bila ultrametrika?}

Funkcija \(d\) je \emph{ultrametrika} ako je metrika i ako vazi:

\[d(p, q) \leq max\{d(p, z), d(z, q)\}, \forall p, q, z\]

\section{Koje se mere rastojanja cesto koriste za kvantitativne podatke?}

Hamingovo rastojanje, rastojanje Minkovskog, Mahalanobisovo rastojanje.

\section{Sta je rastojanje Minkovskog (prednosti/nedostaci)?}

Za dva objekta \(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\)
Rastojanje minkovskog se definise kao:

\[\sum_{i=1}^{n} {({|x_i - y_i|}^p)}^{1/p}\]

Rastojanje Minkovskog za \(p = 2\) je Euklidsko rastojanje, za \(p = 1\) je Menhetn
rastojanje. Prednost ove metode je u tome sto je veoma intuitivna. Medjutim, to sto je intuitivna,
ne znaci da daje dobre rezultate, pogotovo u slucajevima velike dimenzionalnosti. Na primer ne uzima
u obzir koliko je neki atribut bitan za odredjivanje slicnosti. Takodje lose radi ako je nepoznata
raspodela\ldots

\section{Sta je Mahalanobisovo rastojanje (prednosti/nedostaci)?}

Jedan od nedostataka rastojanaj Minkovskog je sto zavisi samo od objekata nad kojim se formula
izracunava, a ne obraca paznju na distribuciju ostalih podataka. Mahalanobisovo rastojanje uzima u
obzir raspodelu podataka koristeci matricu kovarijansi. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) dva objekta.
Mahalanobisovo rastojanje izmedju \(\overline{X}\) i \(\overline{Y}\) je:

\[
    Maha(\overline{X}, \overline{Y}) =
    \sqrt{
        (\overline{X} - \overline{Y}) \times \Sigma^{-1} \times (\overline{X} - \overline{Y})^{T}
    }
\]

Drugim recima uzimamo razliku vektora \(\overline{X}\) i \(\overline{Y}\) pomnozimo sa inverzom
matrice kovarijansi \(\Sigma\) i transponovanom razlikom vektora \(\overline{X}\) i \(\overline{Y}\).

\section{Kako se moze definisati slicnost podataka sa kategorickim atributima?}

Kad radimo sa kategorickim funkcijama obicno se vise koriste funkcije slicnosti nego razlicitosti
jer je se diskretne vrednosti mogu prirodnije porediti.  Slicnost podataka sa kategorickim
atributima se moze definisati preko slicnosti njihovih pojedinacnih atributa. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti. Njihovu
slicnost mozemo definisati kao:

\[
    Sim(\overline{X}, \overline{Y}) = \sum_{i = 1}^{n} S(x_i, y_i)
\]


Odavde vidimo da izbor funkcije \(S\) odredjuje citavu funkciju slicnosti. U najjednostavnijem
slucaju funkcija \(S\) se moze definisati kao

\[
    S(x_i, y_i) =
    \begin{cases}
        1 & x_i = y_i \\
        0 & x_i \neq y_i \\
    \end{cases}
\]

Medjutim, mozemo uvideti da je problem kod ove funkcije da ona ne uzima u obzir frekvenciju
razlicitih atributa. Uzmimo na primer atribut koji moze da ima vrednosti `Normalno', `Rak' i
`Dijabetes'. Najverovatnije je da ce 99\% podataka imati vrednost `Normalno' ali oni nece biti od
statisticke vaznosti isto toliko koliko i objekti sa vrednostima `Rak' i `Dijabetes'. Drugim recima,
velika vecina nam ne odredjuje dovoljno dobro slicnost izmedju objekata. Sa ovim na umu treba
kreirati nesto slicno Mahalanobisovom pristupu. Takav pristup naziva se \emph{inverzna frekvencija
ponavljanja}.

 Neka je \(p_i(x)\) broj slogova ciji \(i\)-ti atribut ima vrednost \(x\). Tada mozemo nasu funkciju
 \(S\) definisati kao:

 \[
    S(x_i, y_i) =
    \begin{cases}
        \dfrac{1}{p_i(x_i)^2} & x_i = y_i \\
        0 & x_i \neq y_i \\
    \end{cases}
 \]

\section{Kako se odredjuje slicnost tekstualnih dokumenata?}
\label{pitanje:slicnost_tekst_dokumenata}

Tekstualne dokumente mozemo smatrati multidimenzionim podacima kada bi ih posmatrali kao `vrece
reci'. To bi znacilo da bi kompletan set atributa nekog dokumenta bio ceo leksikon reci, a vrednosti
bi bile broj pojavljivanja odgovarajuce reci u dokumentu. Ovakav format bi znacio da ce vecina
atributa imati vrednost 0, sto bi dalje povlacilo da kada bismo koristili nesto kao sto je
rastojanje Minkovskog, dva slicna dugacka teksta ce uvek biti vise razlicita nego dva zapravo
razlicita kraca teksta. Da bismo ovo izbegli, koristimo kosinusno rastojanje. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti. Kosinusno
rastojanje definisemo kao:

\[
    \cos(\overline{X}, \overline{Y}) =
    \dfrac{\sum_{i=1}^{n} x_i \cdot y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}
\]

Ova metoda ne uzima u obzir odnos pojavljivanja reci. Mi znamo da su tekstovi slicniji ukoliko se
podudaraju na recima koje se retko javljaju, nego na onim koje se cesto javljaju. \emph{Inverznu
frekvenciju dokumenta} definisemo kao:
\[id_i = \log(n/n_i)\]
gde je \(n\) ukupan broj dokumenata, a \(n_i\) je broj dokumenata u kojima se \(i\)-ta rec
pojavljuje.

Kako pojavljivanje jedne reci ne bi trebalo da poremeti celu meru, koristimo funkcije:

\[f(x_i) = \sqrt{x_i}\]
\[f(x_i) = \log(x_i)\]

\emph{Normalizovane} funkcije frekvencije reci se onda definisu kao

\[h(x_i) = f(x_i) \cdot id_i\]

Sada mozemo napisati kosinusnu meru slicnosti koristeci ove normalizovane funkcije frekvencije
ponavljanja reci

\[
    \cos(\overline{X}, \overline{Y}) =
    \dfrac{\sum_{i=1}^{n} h(x_i) \cdot h(y_i)}{\sqrt{\sum_{i=1}^{n} h(x_i)^2} \cdot
        \sqrt{\sum_{i=1}^{n} h(y_i)^2}}
\]


\section{Sta je rastojanje Minkovskog sa tezinama?}

U nekim slucajevima, nisu svi atributi objekta podjednako bitni u odredjivanju slicnosti. Na primer,
visina plate igra mnogo vecu ulogu nego pol u slucaju odobravanja kredita. U ovakvom slucaju mozemo
koristiti rastojanje Minkovskog sa tezinama (generalizovano rastojanje Minkovskog).

\[
    Sim(\overline{X}, \overline{Y}) = \left(\sum_{i=1}^{n} a_i \cdot |x_i - y_i|^p\right)^{1/p}
\]

Vrednost \(a_i\) nam govori o vaznosti \(i\)-tog atributa u poredjenju dva
objekta. Vrednosti \(a_i\) se dobijaju heuristickim metodama i u velikoj meri zavise od iskustva
analiticara.

\section{Kako se odredjuje slicnost dva sloga sa kvantitativnim i kategorickim atributima?}

Poprilicno jednostavan pristup resavanju ovog problema je da dodamo tezine dobijene za numericke
parametre i tezine dobijene za kategoricke parametre. Neka su
\(\overline{X}=(\overline{X_n}, \overline{X_c})\) i \(\overline{Y}=(\overline{Y_n}, \overline{Y_c})\)
objekti koji sadrze numericke i kategoricke atribute.

\[
    Sim(\overline{X}, \overline{Y}) =
        \lambda \cdot NumSim(\overline{X_n}, \overline{Y_n})
        + (1 - \lambda) \cdot CatSim(\overline{X_c}, \overline{Y_c})
\]

Ovde \(\lambda\) oznacava bitnost kategorickog i numerickog dela u izracunavanju slicnosti. Izbor
\(\lambda\) je tezak, pogotovo sa ogranicenim poznavanjem domena podataka. Generalno se uzima udeo
numerckih atributa u ukupnom skupu atributa, mada ovo ne znaci da je to dobar izbor.

\section{Sta je SMC (simple matching coefficient)?}
SMC (jednostavno uparivanje koeficijenata) se koristi za objekte sa binarnim atributima. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti sa binarnim
atributima. Definisemo
\begin{itemize}
    \item \(M_{01}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{10}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 0 u \(\overline{Y}\)
    \item \(M_{11}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{00}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 0 u \(\overline{Y}\)
\end{itemize}
Tada je
\[SMC(\overline{X}, \overline{Y}) = \dfrac{M_{11} + M_{00}}{M_{00} + M_{11} + M_{10} + M_{01}} \]

\section{Sta su Zakardovi koeficijenti? Kada se koriste?}
Zakardovi koeficijenti se koriste u slucaju objekata sa asimetricnim binarnim atributima. Definise
se kao kolicnik broja parova gde su obe vrednosti ne-nula i broja parova gde nisu obe vrednosti
nula. Neka su \(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti
sa asimetricnim binarnim atributima. Definisemo
\begin{itemize}
    \item \(M_{01}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{10}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 0 u \(\overline{Y}\)
    \item \(M_{11}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{00}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 0 u \(\overline{Y}\)
\end{itemize}
Tada zakardove koeficijente izracunavamo
\[Jacc(\overline{X}, \overline{Y}) = \dfrac{M_{11}}{M_{11} + M_{10} + M_{01}}\]

\section{Sta su prosireni Zakardovi koeficijenti (koeficijenti Tanimoto-a)?}
\emph{Prosireni Zakardovi koeficijenti} se koriste u slucaju kada atributi nisu binarni, ali se
svodi na slucaj \emph{Zakardovih koeficijenata} ukoliko jesu.
\[
    ExtJacc(\overline{X},\overline{Y}) =
    \dfrac{
        \overline{X} \bullet \overline{Y}
    }{
        ||\overline{Y}||^2 + ||\overline{Y}||^2
        - \overline{X} \bullet \overline{Y}
    }
\]

\section{Kako se definise kosinusna slicnost dva objekta? Kada se koristi?}

Pitanje~[\ref{pitanje:slicnost_tekst_dokumenata}]

\section{Sta je korelacija dva objekta?}
Korelacija izmedju dva objekta koji imaju binarne ili neprekidne atribute je mera linearne
zavisnosti izmedju tih atributa.
\[
    \rho_{xy} = \dfrac{cov_{xy}}{\sigma_x \cdot \sigma_y}
\]
Kovarijansa
\[
    cov_{xy} = \dfrac{1}{n-1}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})
\]
Standardna devijacija
\[
    \sigma_x = \sqrt{\dfrac{1}{n-1}\sum_{i=1}^{n} (x_i - \overline{x})^2}
\]
Srednja vrednost
\[
    \overline{x} = \dfrac{1}{n}\sum_{i=1}^{n} x_i
\]

Korelacija je uvek u intervalu \([-1,1]\). Korelaciju vrednosti 1 zovemo perfektna pozitivna
korelacija. Slicno je za negativnu perfektnu korelaciju.


\section{Kako izrazavamo slicnost diskretnih podataka?}
Slicnost diskretnih podataka mozemo predstaviti kao cenu transformacije jednog objekta u drugi.
Za prvih \(i\) atributa iz \(\overline{X}=(x_1,x_2,\ldots,x_n)\) i prvih \(j\) atributa iz
\(\overline{Y}=(y_1,y_2,\ldots,y_n)\), edit rastojanje je:

\[
    Edit(i, j) = min
    \begin{cases}
        Edit(i-1, j) + cena\_brisanja \\
        Edit(i, j-1) + cena\_umetanja \\
        Edit(i-i, j-1) + I_{ij} \cdot cena\_zamene \\
    \end{cases}
\]
Vrednost \(I_{ij}\) je 1 ukoliko su \(i\)-ti simbol u \(\overline{X}\) i \(j\)-ti simbol u
\(\overline{Y}\) isti, inace je 0.

Jos jedan pristup koji se koristi za poredjenje objekata sa diskretnim atributima, je metoda
najduzeg zajednickog podniza, ali takvog da ne moraju elementi biti jedan za drugim.
\[
    LCSS(i, j) =
    \begin{cases}
        \emptyset & i = 0 \lor j = 0 \\
        LCSS(i-1, j-1) & i,j > 0 \land x_i = y_i \\
        longest\{LCSS(i, j-1), LCSS(i-1, j)\} & i,j > 0 \land x_i \neq y_i \\
    \end{cases}
\]

\section{Sta je entropija?}
Entropija dogadjaja \(X\) je
\[
    H(X) = - \sum_{i=1}^{n-1}p_i\log_{2}p_i
\]
Broj \(n\) obelezava broj razlicitih klasa podataka koje razmatramo, a \(p_i\) je broj slogova
koji pripadaju klasi \(i\).

\section{Sta su mere na osnovu gustina? Koje se metode najcesce koriste?}
Mere na osnovu gustina odredjuju stepen bliskosti objekata u nekoj oblasti. Najcesce se koristi pri
klasterovanju, i otkrivanju anomalija. Najcesce se koriste
\begin{itemize}
        \item Euklidska gustina --- broj tacaka po jedinici povrsine/zapremine
        \item Gustina verovatnoca --- procena distribucije podataka na osnovu izgleda
        \item Graf zasnovane gustine --- na osnovu povezanosti
\end{itemize}
Primer euklidske gustine bi bila podela regiona na neki broj celioja i definisanje gustine preko
broja tacaka u celijama.

Jos jedan primer bi mogao biti udaljenost od neke centralne tacke.

\section{Sta je priprema podataka? Zasto se vrsi?}
Ravni podaci su uglavnom veoma razliciti jedni od drugih. Podaci mogu da fale, da budu
nekonzistentni, pogresni itd\ldots Cilj je podatke dovesti u takvo stanje da se na njih mogu
primeniti razliciti pristupi istrazivanja podataka. Drugim recima, cilj name je da izvucemo sve
korisne informacije iz ovih izvora podataka. Neki od pristupa koji se koriste su
\begin{itemize}
    \item Izdvajanje karakteristika
    \item Ciscenje podataka
    \item Izbor i transformacija podataka
    \item Redukcija podataka
\end{itemize}

\section{Sta je izdvajanje karakteristika?}
Ravni podaci su cesto u formi koji nisu dobri za procesiranje. Uzmimo za primer logove aplikacije,
dokumente itd\ldots Cilj nam je da iz ovakvih nestruktuiranih podataka, izvucemo osobine sa kojima
mozemo da radimo, i koje ce nam pomoci u resavanju konkretnog zadatka istrazivanja podataka. Ovo
se moze smatrati i najbitnijim delom celog procesa istrazivanja podataka, jer analiza podataka moze
biti samo onoliko dobra koliko su i podaci dobri.


\section{Sta je prenosivost tipova podataka?}
Osobina podataka uglavnom jeste da su heterogeni. To znaci da objekti imaju atribute razlicitih
tipova. Ovo umnogome ogranicava izbor postojeceg algoritma za obradu podataka. Zbog ovoga postoji
koncept prenosivosti tipova. Naravno, pretvaranjem jednog tipa u drugi, moguce je izgubiti neke
informacije o podacima. Zbog ovoga treba biti pazljiv prilikom primene metoda za prevodjenje tipova.
Neke od popularnih metoda su

\begin{itemize}
    \item Diskretizacija
    \item Binarizacija
    \item Latentna Semnticka Analiza (LSA)
    \item Simbolicka aproksimacija agregata (SAX)
    \item Diskretna Furijeova Transformacija (DFT)
    \item Diskretna Transformacija Talasicima (DWT)
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Tablela prevodjenja tipova}
    \begin{tabular}{||c|c|c||}
        \hline
        \textbf{Izvorni tip} & \textbf{Ciljni tip} & \textbf{Metode} \\ \hline \hline
        Numericki & Kategoricki & Diskretizacija \\ \hline
        Kategoricki & Numericki & Binarizacija \\ \hline 
        Tekst & Numericki & LSA \\ \hline
        Vremenski intervali & Diskretne niske & SAX \\ \hline
        Diskretne niske & Numericki multidimenzioni & DWT, DFT \\ \hline
        Vremenski intervali & Numericki multidimenzioni & DWT, DFT \\ \hline \hline
    \end{tabular}
\end{table} 

\section{Sta je diskretizacija? Kada se koristi? Kako se biraju intervali?}
\label{sect_diskretizacija}
Najcesce transformisemo podatke iz numerickih u kategoricke. Ovaj proces nazivamo
\emph{diskretizacija}. Osnovni pristup je da numericke podatke podelimo u intervale, svakom
intervalu pridruzimo jednu diskretnu vrednost i onda atribute sa numerickom vrednoscu iz nekog
intervala prevedemo u odgovarajucu kategoricku vrednost za taj interval. Na primer godine podelimo
na \([1, 10]\), \([11, 20]\), \([21, 30]\) i intervale preslikamo u diskretne vrednosti, na primer,
\(A\), \(B\), \(C\) onda ce simbolicka vrednost svakog objekta sa godinama u intervalu \([1, 10]\)
biti \(A\)\ldots Posto se podaci u jednom intervalu ne razlikuju, diskretizacijom gubimo informacije
ali se ispostavlja da ovaj gubitak nije od presudnog znacaja za analizu podataka. Nabrojacemo
nekoliko nacina biranja intervala.
\begin{enumerate}
    \item \emph{Jednake sirine intervala} --- Interval podelimo na \(n\) delova jednake duzine.
        Svaki interval \([a, b]\) se bira tako da je \(b - a\) jednako za svaki interval. Ovaj
        pristup nije dobar, ukoliko raspodela vrednosti atributa nije uniformna.
    \item \emph{Jednaki logaritmi sirine intervala} --- Svaki interval \([a, b]\) se deli na nacin
        da je \(\log(b) - \log(a)\) jednako za svaki interval. Ovaj pristup ima efekat da se
        intervali geometrijski sire \([a, a \cdot \lambda]\),
        \([a\cdot \lambda, a \cdot \lambda^2]\), i tako dalje za \(\lambda > 1\)
    \item \label{podela_intervala:jednak_br_el} \emph{Jednak broj elemenata u intervalu} --- Ideja
        je da se intervali odrede tako da svaki interval sadrzi jednak broj objekata. Ovo se moze
        postici sortiranjem vrednosti atributa, i onda odredjivanjem tacaka podele nad sortiranim
        nizom.
\end{enumerate}

\section{Sta je binarizacija? Kada se koristi?}
\emph{}Binarizacija je postupak transformacije kategorickih podataka u numericke radi lakse primene
nekih algoritama. Ako kategoricki atribut ima \(n\) razlicitih vrednosti, onda kreiramo \(n\)
binarnih atributa. Svaki binarni atribut odgovara jednom kategorickom. Dakle, samo jedan binarni
atribut ce imati vrednost 1. Binarizacija se obicno primenjuje na atribute u analizi zasnovanoj na
pravilima
pridruzivanja.

\section{Kako se tekstualni podaci prevode u numericke?}
Iako vektorska reprezentacija teksta moze da se posmatra kao redak vektor ogromne dimenzionalnosti,
ovaj pristup nije najbolji. Uglavnom za ovakav format je bolje koristiti, na primer, kosinusnu
slicnost, nego Euklidsko rastojanje. U svakom slucaju, bilo bi dobro kada bismo mogli da koristimo
algortme koji rade sa numerickim podacima.

Prvi korak u transformisanju je koriscenje \emph{latentne semanticke analize (LSA)} kako bismo tekst
pretvorili u vektor koji nije redak i ima nizu dimenzionalnost. Nakon toga, svaki dokument
\(\overline{X} = (x_1, \ldots, x_n)\) mora biti skaliran na
\(\dfrac{1}{\sqrt{\sum_{i=1}^{n}x_i^2}}(x_1, \ldots, x_n)\). Ovo skaliranje je neophodno da bi
osiguralo da se dokumenti razlicitih duzina tretiraju na uniforman nacin. Nakon ovog skaliranja,
mere poput Euklidskog rastojanja su efektivnije.

\section{Kako se podaci iz vremenskih serija prevode u diskretne niske?}
Vremenske serije se mogu prevesti u diskretne niske pomocu \emph{simbolicke aproksimacije agregata
(SAX)}. Ova metoda se sastoji iz dva koraka
\begin{enumerate}
    \item Podelimo vremensku seriju na \(x\) jednakih delova, pa onda za svaki deo izracunamo
        prosecno vreme.
    \item U ovom koraku diskretizujemo vrednosti pomocu podele
        intervala~\ref{podela_intervala:jednak_br_el}. Umesto da sortiramo elemente vremenske serije
        pretpostavljamo da vrednosti u vremenskim serjiama imaju normalnu (Gausovu) raspodelu.
        Koristeci standardnu devijaciju i srednju vrednost, odredjujemo parametre raspodele. Kao
        granice intervala za diskretizaciju uzimamo kvantile ove raspodele (uglavnom 3 do 10). Ovim
        smo napravili simbolicku reprezentaciju vremenske serije, koja je sad zapravo diskretna
        niska.
\end{enumerate}

\section{Kako se diskretne niske prevode u numericke podatke?}
Transformacija se vrsi u dva koraka
\begin{enumerate}
    \item Za svaki simbol iz diskretne sekvence se kreira jedan red binarnih vrednosti, duzine
        originalne sekvence, gde broj 1 nalazi na svakom mestu na kome se taj simbol nalazi u
        originalnoj sekvenci. Primer:
        \begin{table}[H]
            \centering
            \begin{tabular}{ccccc}
                \hline
                A & B & B & A & C \\ \hline
                1 & 0 & 0 & 1 & 0 \\
                0 & 1 & 1 & 0 & 0 \\
                0 & 0 & 0 & 0 & 1 \\
            \end{tabular}
        \end{table} 
    \item U ovom koraku se na dobijenu vrednost primenjuje transformacija talasicima, i na kraju se
        osobine iz razlicitih serija spajaju kako bi se dobio jedan multidimenzioni slog.
\end{enumerate}

\section{Koji su aspekti ciscenja podataka?}
Razni faktori mogu da uticu na to da imamo nepotpune ili nekorektne podatke. Na primer, ljudi ne
zele da daju podatke iz privatnih razloga, ako su u pitanju senzori, moze se desiti da se desi
problem u prenosu podataka, mogu biti u pitanju ljudske greske pri rucnom unosu podataka\ldots Zbog
ovoga je ciscenje podataka neophodno. Aspekti ciscenja podataka su:
\begin{enumerate}
    \item Rad sa nedostajucim podacima
    \item Rad sa nekorektnim podacima
    \item Rad sa dupliranim podacima
    \item Skaliranje i normalizacija
\end{enumerate}

\section{Sta podrazumeva rad sa nedostajucim podacima?}
Nedostajuci podaci su cesti u bazama podataka gde metod sakupljanja podataka nije savrsen, Na primer
cesto se desava da na anketama ne budu odgovorena sva pitanja. Tri skupa tehnika se koristi kako bi
se prevazisao problem nedostajucih podataka:
\begin{enumerate}
    \item Ako nekom objektu nedostaje podataak, brisemo ga. Ovaj pristup nije toliko dobar ako su
        objekti sa nedostajucim vrednostima cesti
    \item Nedostajuce vrednosti se mogu pretpostaviti \emph{(imputacija)}. Imputacija, doduse, moze
        da utice na rezultate istrazivanja podataka.
    \item Algoritmi se modifikuju tako da mogu da podnesu nedostajuce vrednosti.
\end{enumerate}
Problem predvidjanja nedostajucih vrednosti je direktno vezan sa problemom klasifikacije, s tim sto
problem klasifikacije se bavi odredjivanjem jednog specijalnog atributa na osnovu ostalih, dok u
ovom slucaju vise vrednosti moze nedostajati, pa je time problem slozeniji.

\section{Sta podrazumeva rad sa nekorektnim podacima?}
Kljucne metode za otklanjane ili korekciju nekorektnih vrednosti su:
\begin{enumerate}
    \item \emph{Detekcija nekonzistentnosti} --- uglavnom je greska vidljiva u koliko imamo vise
        izvora podataka gde su podaci koji se odnose na istu vrednost razliciti.
    \item \emph{Domensko znanje} --- na primer, mozemo da znamo da ako polje vezano za drzavu ima
        vrednost `Srbija', polje glavni grad ne moze biti `London'.
    \item \emph{Metode orijentisane podacima} --- statisticko ponasanje podataka se moze iskoristi u
        pronalazenju autlajera. Treba obratiti paznju da ne mora uvek da znaci da su ove vrednosti
        anomalije, te se moraju rucno ispitati pa tek onda potencijalno odbaciti.
\end{enumerate}


\section{Sta podrazumeva rad sa dupliranim podacima?}
\section{Kada se i kako primenjuju skaliranje i normalizacija?}
I mnogo slucajeva, razliciti osobine su razlicito skalirane. Na primer broj godina i visina plate.
Visina plate je mnogo veca od broja godina, sto bi znacilo da bi rezultati primene bilo kog metoda
agregiranja podataka bili odredjeni vecinom od strane atributa sa vecim vrednostima.

Da bismo resili ovaj problem primenjujemo \emph{standardizaciju}. Neka \(j\)-ti atribut ima srednju
vrednost \(\mu_j\) i standardnu devijaciju \(\sigma_j\) i neka je \(\overline{X_i}\) \(i\)-ti slog.
Atribut \(x_{ij}\) mozemo skalirati kao:
\[
    z_{ij} = \dfrac{x_{ij} - \mu_j}{\sigma_j}
\]

Drugi pristup je da koristimo \emph{min, maks} skaliranje, kako bismo sveli atribute na vrednosti u
intervalu \([0, 1]\). Neka je \(\overline{X_i}\) \(i\)-ti slog i neka su \(min_j\) i \(max_j\) redom
minimalna i maksimalna \(j\)-og atributa. Atribut \(x_{ij}\) mozemo skalirati kao:
\[
    z_{ij} = \dfrac{x_{ij} - \min_j}{max_j - min_j}
\]
Ovaj pristup nije efektivan ukoliko su minimum i maksimum neki ekstremni autlajeri. Zamislimo
situaciju gde se greskom na broj godina dodala jos jedna nula, pa imamo maksimalni broj godina 800.
Ovo bi znacilo da ce se svi podaci nalaziti u intervalu \([0, 0.1]\)


\section{Zasto se podaci redukuju i transformisu? Nabrojati pristupe.}
Cilj smanjenja podataka je da se oni predstave kompaktnije. Kada je velicina podataka manja mnogo je
lakse primeniti algoritme na njih, pogotovo oni vece slozenosti. Redukcija podataka se moze vrsiti
na nivou slogova kao i na nivou atributa. Neke od metoda redukcije podataka su:
\begin{enumerate}
    \item Agregacija
    \item Uzimanje uzoraka
    \item Izbor karakteristika
    \item Redukcija podataka pomocu rotacije osa
    \item \ldots
\end{enumerate}

\section{Sta je agregacija? Koja je svrha agregacije?}
Agregacija je kombinovanje dva ili vise atributa (ili objekata) u jedan atribut (objekat). Svrha
agregacije je redukcija podataka (smanjivanje broja atributa ili objekata), promena skale,
stabilniji podaci (podaci koji su agregirani imaju tendenciju da imaju manja odstupanja)

\section{Sta je uzimanje uzoraka? Koji su tipovi uzoraka?}
Uzimanje uzoraka je `filtriranje' originalnih slogova u cilju kreiranja manje baze podataka.
Statisticari biraju uzorke jer je dobijanje kompletnog skupa podataka koji su od interesa jako skupo
i vremenski zahtevno. Izbor uzoraka se koristi kada je obrada kompletnog skupa podataka jako skupa
i/ili vremenski zahtevna.  Glavna prednost uzimanja uzoraka je sto je jednostavno, intuitivno i
relativno lako se implementira.

Kljucni principi za efektivan izbor uzoraka je da koriscenjem uzoraka treba da se dobije efekat
skoro isti kao da je radjeno na kompletnom skupu podataka. Uzorak je reprezentativan ako ima
proksimativno iste osobine kao i originalni skup podataka.

Generalno, razlikujemo dva razlicita slucaja pri uzimanju uzorka.
\begin{enumerate}
    \item Uzimanje uzoraka nad statickim podacima
    \item Uzimanje uzoraka nad tokom podataka
\end{enumerate}

\paragraph{Uzimanje uzoraka nad statickim podacima} se moze izvrsiti na vise nacina:
\begin{enumerate}
    \item \emph{Jednostavan slucajni uzorak } --- uzimamo elemente slucajnim izborom iz originalnog skupa
        podataka. Mozemo svaki element birati sa nekom sansom i time spreciti pojavu duplikata (osim
        u slucaju kada originalni skup podataka sadrzi duplikate), a mozemo svaki put birati iz
        celog skupa podataka i time ostaviti mogucnost pojave duplikata
    \item \emph{Pristrasno uzimanje uzoraka} --- neki podaci su vazniji od drugih, na primer podaci
        koji su skorije skupljeni imaju vece sanse da budu ukljuceni u uzorak.
    \item \emph{Stratifikovano uzimanje uzorka} --- podaci se dele na slojeve i iz svakog sloja se
        bira jednostavan slucajni uzorak. Ovaj pristup se koristi u situacijama kao sto su na primer
        kada pokusavamo da izmerimo ekonomske razlike zivota razlicitih pojedinaca neke populacije.
        Cak ni uzorak od milion ljudi mozda nece obuhvatiti nekog milionera. Zato se podaci
        raslojavaju po zaradi, i onda se svaki sloj nezavisno obradjuje.
\end{enumerate}

\paragraph{Uzimanje uzoraka nad tokom podataka} nekada podaci nad kojima moramo da uzmemo uzorak
nisu poznati u celosti jer su ogromnih razmera i ne mogu da stanu na disk. Dakle, moramo koristiti
neku metodu koja ce dinamicki i efikasno \emph{odrzavati} uzorak. To se svodi na dve odluke
\begin{enumerate}
    \item Kako odluciti da li prispeli objekat iz toka staviti u uzorak.
    \item Kako odluciti koji element iz trenutnog uzorka izbaciti kako bismo `napravili mesta' za
        novi objekat.
\end{enumerate}

\section{Sta je izbor karakteristika?}
Neke atribute mozemo jednostavno odbaciti zato sto nisu od vaznosti. Koji to atributi nisu od
vaznosti? To ocigledno zavisi od toga sta zelimo da postignemo analizom. Izbor karakteristika
takodje smanjuje broj redudantnih karakteristika (na primer broj poena i ocena) kao i dimenzionalnost
podataka. Veliki broj tehnika postoji, pogotovu za klasifikaciju.

Atributi ne moraju samo da se eliminisu. Cesto se formiraju novi atributi koji ukljucuju vazne
karakteristike zbog efikasnije obrade.

\section{Kako se podaci redukuju pomocu rotacije osa?}
TBD

\section{Sta je Principal Component Analysis (PCA)?}
TBD

\section{Sta je Singular Value Decomposition?}
TBD

\section{Navesti jos neke metode dimenzione redukcije?}
TBD

\section{Sta je klasifikacija?}
Klasifikacija je zadatak ucenja \textbf{ciljne funkcije} \(f\) koja mapira skup atributa \textbf{x}
u neku od predefinisanih oznaka klase \(y\).

Klasifikacioni model moze da sluzi za razlikovanje objekata razlicitih klasa, na primer model koji
nam objasnjava razliku izmedju sisara, reptila, ptica, riba. Takodje se moze i koristiti kao model
za predvidnjanje. Klasifikacioni model se moze koristiti kao crna kutija za odredjivanje oznake
klase nekog objekta na osnovu poznatih atributa.

\section{Sta je klasifikaciona tehnika? Nabrojati par.}
Klasifikaciona tehnika (klasifikator) je sistematicni pristup izgradnji klasifikacioinog modela od
ulaznog skupa podataka. Neki od klasifikatora su:
\begin{itemize}
    \item Drveta odlucivanja
    \item Metode zasnovane na pravilima
    \item Neuronske mreze
    \item Statisticki zasnovane metode
    \item Metode zansovane na podrzavajucim vektorima
\end{itemize}

\section{Kako se dele ulazni podaci klasifikacije?}
Uobicajeni pristup klasifikaciji je da se ulazni podaci klasifikacije se dele na dva skupa.
\emph{Trening} skup, cije su klasne oznake poznate, i \emph{Test} skup na koji se primenjuje
klasifikacioni model, cije su klasne oznake nepoznate.

\section{Sta je indukcija po drvetu odlucivanja? Primer.}
Drvo odlucivanja kao klasifikator je jednostavna klasifikaciona tehnika, a ipak je u sirokoj
upotrebi. Da bismo ilustrovali kako ova tehnika radi, uzmimo primer odredjivanja da li je neki
kicmenjak sisar ili ne. Pristup bi bio postavljanje serije pitanja. Prvo pitanje koje bismo mogli da
postavimo jeste da li je toplokrvna ili hladnokrvna zivotinja. Ako je hladnokrvna, onda definitivno
nije sisar. Ukoliko je toplokrvna, ili je ptica ili je sisar. U tom slucaju, treba da postavimo
naredno pitanje. Da li zenke te vrste radjaju svoje mladunce? One vrste koje radjaju su definitivno
sisari, dok oni koji ne radjaju uglavnom nisu (uz neke izuzetke).

Ovo je primer jednog nacina da se resi problem klasifikacije. Pazljivim postavljanjem pitanja o
atributima nekog test podataka, dolazimo do zakljucka kojoj klasnoj oznaci taj objekat pripada.
Ovakva serija pitanja moze da se organizuje u drvo. Ovakvo drvo ima tri vrste cvora:
\begin{enumerate}
    \item \textbf{Koreni cvor} --- nema ulaznih grana i ima 0 ili vise izlaznih grana
    \item \textbf{Unutrasnji cvor} --- ima tacno jednu ulaznu granu i 2 ili vise izlaznih grana
    \item \textbf{List (terminalni cvor)} --- ima tacno jednu ulaznu granu i nijednu izlaznu granu
\end{enumerate}

U cvorovima koji nisu listovi se nalaze testovi za odredjene atribute. Kretanjem kroz drvo
(odgovaranjem na pitanja postavljena test uslovom u cvoru) krecemo se do listova u kojima se nalazi
oznaka klase, koju pridruzujemo objektu nad kojim smo vrsili klasifikaciju.

\section{Nabrojati bar tri algoritma za klasifikaciju drvetima odlucivanja.}
\begin{itemize}
    \item Hantov algoritam
    \item CART
    \item C4.5
    \item \ldots
\end{itemize}

\section{Opisati Hantov algoritam. Koju algoritamsku strategiju koristi?}
Neka je \(y = {y_1,\ldots,y_n}\) skup oznaka klase. Neka je \(D_t\) skup objekata koji se nalaze u
cvoru \(t\) drveta odlucivanja. Ako svi objekti iz \(D_t\) pripadaju klasi \(y_i\), tada je cvor
\(t\) list koji iznacavamo klasom \(y_i\). Ukoliko objekti pripadaju razlicitim klasama, koristimo
\emph{test atribut} kako bismo skup podelili na podskupove, i onda za svaki podskup kreiramo novi
cvor i primenjujemo algoritam rekurzivno.

Hantov algoritam ce raditi ako trening skup sadrzi sve kombinacije atributa, i svaka kombinacija ima
jedinstvenu oznaku klase. Ove pretpostavke su previse rigorizne za vecinu prakticnih situacija.
\begin{enumerate}
    \item Ukoliko se desi da novi cvor, kreiran podelom po test atributu, sadrzi prazan skup
        objekata, tojest, da ni jedan trening objekat nema takvu kombinaciju vrednosti atributa,
        takav cvor se proglasava za list sa klasom koja odgovara klasi cvora roditelja (kako
        roditelj cvor nije list, tako on nema odredjenu klasu, te se za njegovu klasu uzima ona
        kojoj pripada vecina objekata u tom cvoru)
    \item Moguce je da svi objekti u jednom cvoru imaju sve iste vrednosti atributa, sem vrednosti
        klase. U ovom slucaju se cvor proglasava za list i oznaka klasa koja mu se pridruzuje
        odgovara oznaci klase koju ima vecina objekata u tom cvoru.
\end{enumerate}

\section{Koje odluke treba doneti pri indukciji po drvetu odlucivanja?}
Dve odluke koje algoritam ucenja za indukciju po drvetu odlucivanja mora doneti su:
\begin{enumerate}
    \item \textbf{Kako se trening podaci dele?} --- Svaki rekurzivni korak treba da odredi uslov
        podele po nekom atributu. Za implementaciju ovog koraka, neophodno je da algoritam da metodu
        za odredjivanje uslova podele za razlicite tipove atributa, kao i objektivnu meru za procenu
        kvaliteta izbora.
    \item \textbf{Kada se deoba treba zaustavti} --- Deoba se moze zaustaviti na primer kada svi
        objekti pripadaju istoj klasi, ili kada svi objekti imaju jednake vrednosti atributa.
\end{enumerate}

\section{Kako odrediti uslove testiranja za atribute?}
Uslov za deobu zavisi od tipa atributa.

\paragraph{Binarni atributi} Test uslov za binarne atribute generise dva potencijalna rezultata

\paragraph{Imenski atributi} Imenski atributi mogu imati vise razlicitih vrednosti. Nacin za deobu
imenskih atributa moze se izraziti na dva nacina:
\begin{enumerate}
    \item\label{deoba_imenski} podela na vise grana, i
    \item\label{deoba_binarni} binarna podela.
\end{enumerate}
Primer za bi bio ako imamo atribut \emph{bracni status} koji moze da ima
vrednosti \emph{neozenjen}, \emph{ozenjen} i \emph{razveden}. U slucaju~\ref{deoba_imenski} bismo
delili objekte na 3 dela, a u slucaju~\ref{deoba_binarni} mozemo objekte podeliti na primer
\emph{\{ozenjen\} \{neozenjen, razveden\}} ili na neku drugu od mogucih \(2^{k-1}-1\) mogucih
kombinacija (gde je \(k\) ukupan broj vrednosti koje atribut moze da ima).

\paragraph{Redni atributi} Slicno kao imenski, redni atributi se mogu podeliti na onoliko razlicith
vrednosti koliko razlicitih vrednosti atribut moze da ima, ili binarno. Stvar koja se razlikuje je
da prilikom binarne podele, moramo da pazimo da ne narusimo poredak vrednosti atributa. Na primer
ako imamo redni atribut \emph{velicina majice} i on moze da ima vrednosti \emph{small},
\emph{medium}, \emph{large} i \emph{extra large}, \emph{validne} binarne podele bi bile
\begin{itemize}
    \item \emph{\{small, medium\}, \{large, extra large\}}
    \item \emph{\{small\}, \{medium, large, extra large\}},
    \item \ldots
\end{itemize}
 
dok bi \emph{nevalidna} podela bila
\begin{itemize}
    \item \emph{\{small, extra large\}, \{medium, large\}}
\end{itemize}

\paragraph{Neprekidni atributi} Opet mozemo izvrsiti podelu binarno i na vise grana. U slucaju
binarne podele, biramo objekte ciji atribut zadovoljava \(A < v\) ili  \(A \geqslant v\), pritom,
moramo ispitati sve moguce vrednosti \(v\) po kojima se moze vrsiti podela, i izabrati najbolju. U
slucaju podele na vise grana, biramo \(v_i \leqslant A < v_{i+1}\) za \(i=1,\ldots,k\). Prilikom
ovog pristupa moramo izabrati sve moguce intervale i izabrati najbolji za podelu. Jedan od pristupa
koji ovo omogucava je diskretizacija~\ref{sect_diskretizacija}.

\section{Nabrojati mere koje se koriste za odredjivanje necistoce cvora. Kako se ove mere koriste
pri odredjivanju kvaliteta uslova testiranja?}
Mere koje se koriste za odredjivanje najbolje podele su cesto bazirane na meri necistoce cvorova
koje podela kreira. Na primer cvor sa podelom (0, 1) (0 cvorova pripada prvoj klasi, jedan pripada
drugoj klasi) ima necistocu nula, dok cvor (1, 1) ima najvecu necistocu. Primeri mere necistoce
cvora su:
\begin{enumerate}
    \item Entropija
    \item Gini
    \item Greska u klasifikaciji
\end{enumerate}

Da bismo utvrdili koliko je neka podela dobra, mozemo iskoristiti sledecu formulu
\begin{equation}
    \label{eq:dobit}
    \Delta = I(roditelj) - \sum_{j=1}^{k} \dfrac{N(v_j)}{N}I(v_j)
\end{equation}

gde je \(I\) mera necistoce cvora, \(k\) je broj dece cvorova a \(N(v_j)\) je broj objekata u
detetu cvoru \(v_j\). Sto je \(\Delta\) vece, to je podela kvalitetnija.

\section{Opisati detaljno GINI meru necistoce cvora.}
Neka je \(p(i|t)\) (nekada mozemo oznacavati i samo kao \(p_i\)) udeo objekata koji pripada klasi
\(i\) u nekom cvoru \(t\). Drugim recima:
\[
    \dfrac{broj\_objekata\_klase\_i}{ukupan\_broj\_objekata\_u\_cvoru\_t}
\]

Neka je \(c\) broj razlicitih oznaka klase. Ginijeva mera necistoce cvora je
\begin{equation}\label{eq:gini}
    Gini(t) = 1 - \sum_{i=0}^{c-1} [p(i|t)]^2
\end{equation}

\paragraph{Podela binarnih atributa} Ova podela nije toliko komplikovana. Za svaki binarni atribut
napravimo podelu, izracunamo Ginijev indeks za svaku od podela. Atribut koji odgovara podeli sa
najmanjom vrednoscu Ginijevog indeksa se bira za podelu.

\paragraph{Podela imenskih atributa} U slucaju da delimo imenske atribute binarno, isto postupamo
kao pri podeli binarnih atributa tojest za svaku kombinaciju vrednosti imenskih atributa racunamo
Ginijev indeks.

U slucaju da delimo na vise grana, racunamo Ginijev indeks za svaku mogucu vrednost. Mozemo
uporediti da li je bolje da delimo binarno ili na vise grana uporedjivanjem Ginijevih indeksa.

Izracunavamo kvalitet podele po formuli za svaki nominalni atribut, i biramo atribut, kao i to da li
cemo deliti binarno ili na vise grana, na osnovu izracunatih indeksa.

\paragraph{Podela neprekidnih atributa} Pristup grube sile ovom problem bi bio da se posmatraju sve
vrednosti atributa kao potencijalni kandidati za deobu. Izracunamo Ginijev indeks za svaku od ovih
vrednosti i izaberemo vrednost koja ima najmanju vrednost Ginijevog indeksa. Ovaj pristup je
neefikasan jer ima slozenost \(O(n^2)\). Drugi pristup bi bio da sortiramo moguce vrednosti podele,
sto je slozenosti \(O(n\log n\), i onda mozemo linearnim prolaskom, pametno odredjivati Ginijeve
indekse na osnovu prethodnih.

\section{Opisati Entropiju kao meru necistoce cvora.}
Entropija se racuna kao
\[
    Entropy(t) = - \sum_{i=0}^{c-1} p(i|t)log_2 p(i|t)
\]

\emph{Maksimum} ove funkcije je \(\log n_c\), kada su cvorovi ravnomerno distribuirani. Ovo sadrzi
najmanje zanjimljivih informacija. \emph{Minimum} ove funkcije je \(0\) sto znaci da svi atributi
pripadaju jednoj klasi. Ovo sadrzi najvise zanimljivih informacija.

\section{Opisati gresku u klasifikaciji kao meru necistoce cvora.}
Mera necistoce cvora zasnovana na gresci klasifikacije cvora se racuna kao
\[
    Error(t) = 1 - \max_i[p(i|t)]
\]
Maksimum je \(1-1/n_c\) kada su slogovi ravnomerno raspodeljeni u svim klasama sadrzi najmanje
interesantne informacije. Minimum je \(0\) kada svi slogovi pripadaju jednoj klasi, sadrzi
najinteresantnije informacije.

\section{Sta je odnos dobiti?}
Mere necistoce kao sto su entropija i Ginijev indeks favorizuju atribute koji imaju veliki broj
razlicitih vrednosti. Ova cinjenica moze da predstavlja problem u slucaju kada imamo na primer
atribut koji predstavlja identifikacioni broj korisnika. Identifikacioni broj korisnika nije dobar
atribut za predvidjanje jer je on jedinstven za svakog korisnika. Postoje dve startegije za
resavanje ovog problema.  Ograniciti podelu da bude uvek binarna. Koristi se u algoritmu \emph{CART}
Druga strategija jeste da se modifikuje kriterijum deobe tako da uracuna broj rezultata koji se
dobijaju deobom. Na primer, u algoritmu \emph{C4.5} kriterijum deobe koji se jos naziva i
\textbf{odnos dobiti} (eng.\ gain ratio) se koristi kako bi se utvrdio kvalitet podele. Odnos dobiti
se racuna kao:
\begin{equation}
    \label{eq:odnos_dobiti}
    OdnosDobiti = \dfrac{\Delta_{info}}{SplitInfo}
\end{equation}
Gde je \(SplitInfo = -\sum_{i=1}^{k}\dfrac{N_i}{N}\log_2\dfrac{N_i}{N}\), \(k\) je broj deoba, \(N\)
je broj objekata u roditelj cvoru, a \(N_i\) je broj cvorova u detetu cvoru.

Ovo znaci da ukoliko deobom po nekom atributu dobijemo veliki broj deoba, ona ce imati veci
\(SplitInfo\) i samim tim manji \(OdnosDobiti\)

\section{Koji su kriterijumi za zaustavljanje indukcije po drvetu?}
Sirenje drveta se zaustavlja kada svi slogovi pripadaju istoj klasi
\begin{itemize}
    \item Sirenje drveta se zaustavlja kada svi slogovi pripadaju istoj klasi
    \item Sirenje drveta se zaustavlja kada svi slogovi imaju iste vrednosti atributa
    \item Ranije zaustavljanje
\end{itemize}

\section{Koji su prakticni problemi pri klasifikaciji?}
\begin{itemize}
    \item Preprilagodjavanje i potprilagodjavanje
    \item Nedostajuce vrednosti
    \item Cena klasifikacije
\end{itemize}

\section{Kako se dele greske pri klasifikaciji?}
Greske nacinjene od strane klasifikacionog modela su podeljene u dva tipa: \textbf{greske pri
treniranju} i \textbf{greske u uopstavanju}. Greska pri treniranju, takodje poznata kao
\textbf{greske resupstitucije} je broj misklasifikacija nad podacima za treniranje, dok je greska u
uopstavanju broj ocekivanih gresaka nad nepoznatim podacima. Nama je cilj da i greske pri treniranju
kao i greske uopstavanja budu sto manje.

\section{Sta je preprilagodjavanje a sta potprilagodjavanje modela?}
\paragraph{Preprilagodjavanje modela} Model sa malom greskom na trening podacima moze imati vecu
gresku pri generalizaciji od nekog modela koji ima manju gresku prilikom procesa treninga. Postoje
razni razlozi za preprilagodjavanje modela.
\begin{enumerate}
    \item \textbf{zbog suma} --- trening podaci mogu sadrzati nege greske,
        odnosno sum.  Ukoliko napravimo model koji radi bez greske na trening podacima, on ce
        obuhvatiti ove greske, i pri generalizaciji moze da pogresno klasifikuje one neke objekte
        koji dele vrednosti atributa sa onim objektima koji su pogresni u trening podacima.
    \item \textbf{zbog nedostatka reprezentativnih uzoraka} --- modeli koji svoje odluke donose na
        osnovu malog trening skupa, takodje su podlozni preprilagodjavanju. Takvi modeli mogu biti
        generisani zbog nedostatka reprezentativnih uzoraka iz trening skupa.
\end{enumerate}

\paragraph{Potprilagodjavanje modela} Ako je model isuvise jednostavan i greska pri treniranju i
greska pri uopstavanju mogu da budu jako visoke.

\section{Sta je procena greske u generalizaciji? Zasto se koristi?}
Preprilagodjavanje kod modela zasnovanih na drvetu odlucivanja se javljaju kada je drvo odlucivanja
slozenije nego potrebno. Pitanje je kako mozemo da odredimo dobru slozenost modela. Idealna
slozenost modela je ta koja daje najmanju gresku pri generalizaciji. Problem je sto algoritam ima
pristup samo skupu trening podataka dok gradi model. Takodje nema nikakvo znanje o test skupu pa
zato ne zna koliko dobro ce se ponasati na podacima koje nije video pre. Dakle treba nam metoda za
procenu greske.
\paragraph{Koriscenje greske resupstitucije} Ovaj pristup pretpostavlja da trening skup dobro
predstavlja celokupne podatke. Pri ovom optimistickom pristupu, algoritam jednostavno bira model
koji izaziva najmanju gresku nad trening podacima. Ovo je uglavnom los metod.

\paragraph{Okamova ostrica} Kao sto smo vec pomenuli, preprilagodjavanje se povecava kako model
postaje slozeniji. Iz ovog razloga treba da preferiramo jednostavnije modele. Ovo je dobro poznata
strategija Okamove ostrice. Ako nam je dato dva modela sa istom greskom generalizacije, biramo
jednostavniji.

\paragraph{Pesimisticki pristup} Neka je \(n(t)\) broj trening podataka koja se klasifikuje od
strane cvora \(t\) i neka je \(e(t)\) broj lose klasifikovanih podataka. Pesimisticka procena drveta
odlucivanja \(T\), \(e_g(T)\), moze da se izracuna kao:
\[
    e_g(T) = \dfrac{\sum_{i=1}^{k}[e(t_i)+\Omega(t_i)]}{\sum_{i=1}^{k}n(t_i)}
        = \dfrac{e(T) + \Omega(T)}{N_t}
\]
gde je \(k\) broj listova, \(e(T)\) ukupan broj trening gresaka drveta odlucivanja \(N_t\) je broj
trening podataka, a \(\Omega(t_i)\) je cena dodavanja lista cvoru \(t_i\)

\paragraph{Sta je princip najmanje duzine opisa (Minimum Description Length --- MDL)?} Ovaj
informaciono-teorijski pristp se naziva princip najmanje duzine opisa. Prikazimo ovaj princip na
primeru. Neka osoba \textbf{A} i osoba \textbf{B} imaju isti skup podataka sa poznatim skupom
atributa \textbf{x}. Dodatno, samo osoba A zna klase podataka, dok osoba B ne zna. Ukoliko bi osoba
A slala informacije osobi B sekvencijalno, takva poruka bi zahtevala \(\Theta(n)\) bitova
informacije. Alternativno, osoba A moze da konstruise klasifikacioni model koji sumira odnos izmedju
\(x\) i \(y\). Model moze biti kodiran u kompaktan zapis pre slanja osobi B. Ako je preciznost
modela 100\%, onda je cena prenosenja jednaka ceni kodiranja modela. Na primer, ako je model prava,
jasno je da je slanje koeficijenata prave beze od slanja svih podataka. Ukupna cena prenosenja je
\[
    Cena(model,podaci) = Cena(model) + Cena(podaci|model)
\]
pri cemu je prvi prvi sabirak cena kodiranja modela, a drugi sabirak oznacava cenu kodiranja
pogresno klasifikovanih instanci. Prema principu MDL, treba traziti model koji minimizira ukupnu
funkciju cene.

\section{Sta je prepotkresivanje? Sta je potkresivanje po zavrsetku?}
\paragraph{Prepotkresivanje (pravilo ranijeg zaustavljanja)} Algoritam se zaustavlja pre nego sto
stablo naraste do maksimalne velicine. Tipicni uslovi zaustavljanja za odredeni cvor su: `zaustavi
se ako sve instance pripadaju istoj klasi' ili `zaustavi se ako su sve vrednosti atributa iste'.
Dodatna ogranicenja mogu biti, na primer, `zaustavi se ako je broj instanci manji od neke unapred
zadate granice', `zaustavi se ako je raspodela instanci nezavisna od raspodele atributa' (na primer,
vidi se primenom \(\chi^2\) testa) ili `zaustavi se ako sirenje tekuceg cvora ne poboljsava meru
cistoce' (na primer, Ginijev indeks ili informaciona dobit).

\paragraph{Naknadno potkresivanje} Drvo odlucivanja raste do krajnjih granica, pa se zatim iseku
cvorovi u drvetu od dna ka vrhu.  Ako se greska generalizacije poboljsa posle odsecanja poddrvo se
zameni sa cvorom koji je list.  Oznaka klase lista se odreduju prema vecini klasa instanci
poddrveta. Za potkresivanje po zavrsetku se moze koristiti i MDL.

\section{Sta je matrica konfuzije?}
Matrica konfuzije je jedan od nacina evaluacije performansi klasifikacije.
\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    & \multicolumn{3}{|c|}{Predvidjena klasa} \\ \hline
    \multirow{ 3}{*}{Prava klasa} & & C+ & C-- \\ 
    & C+ & TP & FN \\ 
    & C-- & FP & TN \\ \hline
\end{tabular}
\caption{Matrica konfuzije}
\end{table}
Vrednosti u matrici imaju sledeca znacenja \textbf{FP} --- false positive, \textbf{FN} --- false
negative, \textbf{TP} --- true positive, \textbf{TN} --- True negative. C+ i C-- predstavljaju dve
klase jedne kategorije, koje posmatramo kao ,,pozitivno'' i ,,negativno''.
\[
    Preciznost = \dfrac{TP + TN}{TP + TN + FP + FN}
\]
\section{Sta je matrica cene?}
Matrica konfuzije je jedan od nacina evaluacije performansi klasifikacije. Na primer, neka imamo
podatke sa informacijama o kupcima, pri cemu je 99\% kupaca zadovoljno, a 1\% kupaca nzadovoljno.
Zbog toga cemo podstaci da je pogresna klasifikacija kupca koji nije zadovoljan, na primer, deset
puta skuplja za nas nego pogresna klasifikacija kupca koji je zadovoljan. Tabela~\ref{matrica_cene}
prikazuje opstiji primer matrice cene, pri cemu vrednost \(Cena(i|j)\) oznacava cenu pogresne
klasifikacije instance \(j\) kao instanca klase \(i\).
\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    & \multicolumn{3}{|c|}{Predvidjena klasa} \\ \hline
    \multirow{ 3}{*}{Prava klasa} & & C+ & C-- \\ 
    & C+ & Cena(C+|C+) & Cena(C-|C+) \\ 
    & C-- & Cena(C+|C-) & Cena(C-|C-)\\ \hline
\end{tabular}
\caption{Matrica cene}
\label{matrica_cene}
\end{table}

\section{Sta su mere osetljive na cenu?}
Uvedimo dve pomocne mere
\[
    Preciznost = p = \dfrac{TP}{TP+FP},
\]
\[
    Odziv = r = \dfrac{TP}{TP+FN},
\]
Nama treba harmonijska sredina ove dve mere. Harmonijska mera je najostrija. Posledica toga je sto
cim krene da opada preciznost ili odziv, to se veoma znacajno odrazava na \(F\)-meru. Sto
predstavljamo formulom koju jos nazivamo i \(F_1\)-mera
\[
    F-Mera = F = \dfrac{2}{\dfrac{1}{p}+\dfrac{1}{r}} = \dfrac{2TP}{2TP + FN + FP}
\]
\(F\)-mera je specijalan slucaj \(F_{\beta}\)-mere
\[
    F_{\beta} = (1+\beta)^2 * \dfrac{p+r}{\beta^2*p + r}
\]

\section{Opisati algoritam ID3.}
tbd
\section{Opisati algoritam C4.5.}
\subsection{Opis algoritma}
C4.5 je skup algoritama klasifikacije u masinskom ucenju i istrazivanju podataka. Razvio ga je
Quinlan kao naslednika algoritma ID3.  Usmeren je ka nadgledanom ucenju: Ako nam je dat skup
\emph{objekata} koji su opisanih skupom \emph{atributima} i pripadaju nekim \emph{klasama}, C4.5 uci
preslikavanje iz skupa vrednosti atributa u klase, koje moze biti primenjeno za klasifikaciju novih,
do tada nepoznatih objekata.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}

\begin{algorithm}
\caption{C4.5}\label{code:c4.5}
\begin{algorithmic}[1]
    \Require{matrica podataka \emph{D}}
    \Procedure{C4.5}{D}
    \State Tree = \{\}
    \If { svaki element iz \emph{D} pripada istoj klasi \textbf{ILI} neki drugi kriterijum }
    \State terminate
    \EndIf
    \ForAll{atribut a \(\in\) \emph{D}}
        \State Odrediti informacionu dobit deobe po atributu \emph{a}
    \EndFor
    \State \(a_{best}\) = Najbolji atribut za deobu
    \State Tree = Napravi koreni cvor koji testira po \(a_{best}\)
    \State \(D_v\) = Svi podskupovi \(D\) podeljeni po \(a_{best}\)

    \ForAll{\(D_v\)}
        \State \(\text{Tree}_v\) = C4.5(\(D_v\))
        \State Dodati granu ka \(\text{Tree}_v\) iz Tree
    \EndFor
    \State \Return Tree
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Koji su moguci tipovi?} C4.5 nije ogranicen na rukovanje samo binarnim testovima, i
dozvoljava deljenje na dve ili vise grana. Ako je atribut tipa \emph{bool}, test rezultuje dvema
granama. Ako je atribut kategoricki, onda pravimo vise grana, ali takodje razlicite vrednosti mogu
biti grupisane u manje skupove. Ako je atribut numericki, rezultat je opet binaran u formi \(\{\leq
\theta?, > \theta?\}\) gde je \(\theta\) pazljivo izabrana granica izmedju intervala.

\paragraph{Kako se biraju testovi?} C4.5 koristi \emph{entropiju} kao meru,
\emph{dobit}(jednacina~\ref{eq:dobit}) i \emph{odnos dobiti}(jednacina~\ref{eq:odnos_dobiti}) kao
kriterijum podele. Podrazumevani kriterijum je odnos dobiti. U svakoj tacki sirenja drveta, test sa
najboljim kriterijumom se bira pohlepno (ne razmatraju se buduci izbori).

\paragraph{Kako se biraju granice?} Kao sto je vec navedeno, za istinitosne i kategoricke atribute,
vrednosti testa su jednostavno razlicite vrednosti koje taj atribut moze da ima. Za numericke
atribute, granica se bira sortiranjem po tom atributu, pa se svaka deljenjem svaka dva susedna
ispituje koja podela maksimizuje pomenuti kriterijum.

\paragraph{Kako se zaustavlja sirenje stabla?} Grana iz cvora se proglasava da vodi u list ako svaki
objekat koji je pokriven tom granom pripada jednoj klasi. Jos jedan nacin na koji se zaustavlja
sirenje je ako broj objekata padne ispod neke odredjene granice.

\paragraph{Kako se oznake klasa dodeljuju listovima?} Listu se dodeljuje klasa kojoj pripada najvise
objekata.

\subsection{Odlike}
\paragraph{Potkresivanje drveta} Potkresivanje drveta, neophodne je kako bi se izbegla
preprilagodjenost modela. Potkresivanje se izvrsava kada je drvo kompletno, od dna ka vrhu. Postoje
razne taktike. Algoritam \emph{CART} koristi pristup gde se pravi serija stabala, gde se svako
stablo dobija tako sto se neko od podstabala zameni listom. Za biranje lista kojim se menja
podstablo, koristi se mera osetljiva na cenu. Svako stablo se evaluira na skupu test podataka i bira
se ,,najbolje''.

\emph{Smanjenje nivoa greske} je uproscavanje pomenutog pristupa. Koriste se posebni test podaci. Za
svako podstablo koje nije list u originalnom drvetu odlucivanja se procenjuje da li ima koristi da
se to podstablo zameni najboljim mogucim listom. Ako se tako dobijeno stablo ponasa bolje, podstablo
postaje list. Ove zamene se izvrsavaju sve dok naredne zamene ne krenu da zapravo povecavaju gresku
umesto da je smanjuju.

\emph{Pesimisticka procena geske} je inovacija u C4.5 kojoj nije potreban odvojen skup za
testiranje. Ona procenjuje gresku koja bi mogla da se javi na osnovu broja pogresnih klasifikacija u
trening skupu. Ovaj pristup rekurzivno procenjuje gresku koja odgovara cvoru u zavisnosti od
procenjenih greski njegovih grana.

\emph{Intervali poverenja} su prosirenje pesimistickog pristupa. Mozemo modelirati stop greske \(e\)
u listovima  kao Bernulijeve slucajne promenljive. Ako je \(CI\) interval poverenja, gornja granica
\(e_{max}\) moze se odrediti \(e < e_{max}\) sa verovatnocom \(1-CI\) (C4.5 koristi \(CI=0.25\)).

\paragraph{Rukovanje nedostajucim vrednostima} Postoje tri glavna problema:
\begin{enumerate*}[label=(\roman*)]
    \item Kada uporedjujemo atribute radi grananja, a neki imaju nedostajuce vrednosti, kako
        izabrati pravu atribut za deljenje?
    \item Nakon odabira atributa po kome se vrsi grananje, ni jedan objekat iz trening skupa ne moze
        pripadati ni jednom rezultatu testa grananja. Kako tretirati te vrednosti prilikom podele na
        podskupove?
    \item Konacno, kada se drvo iskoristi za klasifikovanje novog objekta, kako da nastavimo niz
        odluka ako drvo vrsi test grananja po atributu koji ima nedostajucu vrednost?
\end{enumerate*}
Primetimo da su prva dva problema vezana za ucenje, dok se treci problem odnosi na aplikaciju drveta
na nove objekte. Postoji vise nacina za resavanje svakog od ovih pitanja.

Za prvi problem mozemo da:
\begin{enumerate}
    \item ignorisemo slucajeve u trening podacima koji imaju nedostajucu vrednost
    \item zamenimo najcescom vrednosti ili medijanom poznatih vrednosti
    \item pomnozimo informacionu dobit sa procentom poznatih vrednosti
    \item popunimo vrednost na osnovu ostalih atributa
\end{enumerate}

Za drugi problem mozemo da podelimo trening skup dok rekuzivno gradimo stablo. U slucaju da se
stablo grana po \(a\) za koje jedan ili vise objekata iz trening skupa imaju nedostajuce vrednosti,
onda mozemo da:
\begin{enumerate}
    \item ignorisemo slucajeve u trening podacima koji imaju nedostajucu vrednost
    \item da se ,,pravimo'' da je objekat imao najcescu vrednost iz skupa vrednosti tog atributa
        \(a\)
    \item pridruziti objekat, svakom podskupu, proporcionalno broju objekata sa poznatim vrednostima
        u svakom podskupu
    \item pridruzimo objekat svakom podskupu
    \item napravimo novu granu za objekte sa nedostajucim vrednostima za \(a\)
    \item odredimo najslicniju vrednost, i dodelimo atributu \(a\)
    \item dodamo objekat u samo jedan podskup, ali opet proporcionalno broju objekata sa poznatim
        vrednostima u tom podskupu
\end{enumerate}

Ukoliko klasifikujemo novi objekat sa nedostajucom vrednosti atributa \(a\), opcije su
\begin{enumerate}
    \item ukoliko postoji podskup sa nedostajucim vrednostima, ubacujemo u taj podskup
    \item granamo se po grani koja ima najcescu vrednost \(a\)
    \item pokusamo da zakljucimo vrednost \(a\) iz ostalih vrednosti, i onda se granamo po dobijenoj
        vrednosti
    \item ubacujemo u podskupove prema relativnoj verovatnoci dobijenoj pretrazivanjem svih mogucih
        ishoda testiranja
\end{enumerate}

\section{Opisati algoritam CART.}
\subsection{Pregled}
CART drvo odlucivanja je binarno rekurzivna procedura deljenja sposobna da obradi neprekidne i
imenske atribute kao ciljne klase i kao atribute na osnovu kojih se deli (?????). Podacima se rukuje
u njihovoj sirovoj formi; nema potrebe za diskretizacijom, niti je ona preporucena. Pocevsi od
korena, podaci se dele na dva deteta. Rekurzivno se dele cvorovi i siri se stablo bez kriterijuma
zaustavljanja, sve dok podela vise nije moguca usled nedostatka podataka. Citavo drvo se naknadno
potkresuje od listova ka korenu metodama osetljivim na cenu. Mehanizam CART algoritma ne proizvodi
jedno drvo, vec seriju ugnjezdenih potkresanih drveta, od kojih je svako kandidat da bude optimalno.
Za razliku od C4.5, CART ne koristi internu meru performansi baziranu na trening skupu podataka, vec
se testiranje vrsi na nezavisnim test podacima.
\subsection{Pojednostavljen pogled na algoritam}

\paragraph{Kreiranje drveta} Potpuno objasnjene CART algoritma, koje ukljucuje sve relevantne
tehnicke detalje, je dugacko i kompleksno; ima vise pravila za deljenje kako za klasifikaciju, tako
i za regresiju, odvojeno rukovanje neprekidnim i kategorickim atributima, nedostajuce
vrednosti\ldots Nakon toga, potkresivanje je takodje jedna kompleksna operacija i konacno, izbor
odgovarajuceg drveta. Dacemo jednostavan pogled na algoritam radi lakseg razumevanja

\begin{algorithm}[H]
    \caption{CART}\label{code:cart}
    \begin{algorithmic}[1]
        \Procedure{CART}{D}
            \State dodeli sve podtake iz \(D\) korenu stabla
            \State definisi koren kao list
            \ForAll{ list \(l\) u stablu:}
                \If {elementi pripadaju istoj klasi ili nema dovoljno elemenata}
                \State goto NEXT
                \EndIf
                \State pronadji atribut koji najbolje razdvaja cvor na dva cvora
                \State NEXT:
            \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{potkresivanje} Kada drvo poraste na svoju maksimalnu velicinu, CART onda generise
ugnjezdenu sekvencu potkresanih stabala. Pojednostavljena verzija potkresivanja bi bila: Uzmi
maksimalno drvo (\(T_{max}\)) i uklanjajuci deobe dodji do dva lista koja ne popravljaju preciznost
drveta nad trening podacima. Ovo je pocetno stanje procedure potkresivanja. Potkresivanje se dalje
nastavlja iterativno uklanjanjem najslabijih karika u drvetu (deobe koje najmanje povecavaju
performanse drveta). Kriterijum koji se koristi za potkresivanje naziva se \emph{potkresivanje
rezanjem troskova} (engl.\ cost-complexity pruning) koja definisemo kao
\begin{equation}\label{eq:cost-complexity-pruning}
    Ra(T) = R(T) + a|T|
\end{equation}
gde \(R(T)\) cena dobijena nad trening uzorkom, \(|T|\) je ukupan broj listova, a \(a\) je kazneni
faktor odredjen od strane cvora. Ako je \(a = 0\) onda je minimalno drvo dobijeno ovim metodom
upravo maksimalno drvo dobijeno algoritmom sirenja.

\subsection{Pravila deljenja}
Pravila deljenja u CART algoritmu uvek imaju formu
\[
    \text{Objekat ide levo ako USLOV, inace ide desno}
\]
gde je USLOV za neprekidne atribute [\(atribut \leq neka\_vrednost\)], a za kategoricke je
[\(atribut \in neki\_skup\_vrednosti\)].

Mere koje se koriste pri deljenju: Gini(formula~\ref{eq:gini}), twoing, uredjeni twoing, simetricni
gini. Twoing je metoda bazirana na uporedjivanju distribucije ciljnog atributa u dva deteta cvora.

\subsection{Prethodne verovatnoce i nebalansirane klase}
Nebalansirane klase su problem jer se vecina metoda istrazivanja podataka lose ponasa nad njima. Na
primer manje od 1\% transakcija kreditnih kartica su transakcije koje predstavljaju nekakvu prevaru.
Ako bismo trening skup pokusali da ogranicimo da ima jednak broj objekata iz obe klase, imali bismo
veoma mali trening skup.

CART se automatski prilagodjava disbalansu. Podaci mogu da se modeliraju kako su pronadjeni, bez
preprocesiranja.

Da bi mogao da pruzi ovakvu fleksibilnost, kartko koristi prethodne verovatnoce. One su ubacene u
metode za odredjivanje kvaliteta podele. U svom podrazumevanom modu klasifikacije, CART uvek racuna
frekvenciju vrednosti po klasama u bilo kom cvoru u odnosu na frekvencije po klasama u korenu. Ovo
je jednako automatskom uravnotezavanju podataka po klasama, i ima za posledicu da ce izabrano drvo
da minimizuje gresku disbalansiranih klasa.

\subsection{Nedostajuce vrednosti}
Jedna od vecih doprinosa CART algoritmu, bio je efektivni mehanizam za automatsko resavanje
nedostajucih vrednosti. Tri situacije u kojima se obradjuju nedostajuce vrednosti:
\begin{itemize}
    \item \textbf{prilikom biranja pogodnog atributa za podelu} --- prvobitne verzije karta su
        ignorisale nedostajuce vrednosti ovih atributa, dok ih naredne verzije uzimaju u obzir.
        Recimo da nedostaje 20\% vrednosti nekog atributa, onda se njegova mera smanjuje za 20\%
    \item \textbf{prilikom smestanja podataka u cvorove}
    \item \textbf{prilikom aplikacije na nove podatke}
\end{itemize}

Za drugi i treci slucaj, CART koristi princip ,,surogata''. Kada se naidje na nedostajucu vrednost
za atribut \(X\), bira se novi atribut \(Y\) po kome ce se objekti sa nedostajucim vrednostima za
\(X\) podeliti. Podela po \(Y\) treba da bude bolja od podele po \(X\), tako da nije uvek moguce
naci surogate. U slucaju da nema surogata, podrazumevano ponasanje dodeljuje objekat vecem detetu
cvoru.

\subsection{Bitnost Atributa}
Bitnost atributa \(X\)se izracunava kao suma dobiti u svakom cvoru gde je atribut \(X\) bio izabran
za podelu, pomnozen procentom broja objekata u cvoru. Surogati se takodje ukljucuju.

\end{document}
