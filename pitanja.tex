% !TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}

\usepackage{float}
\restylefloat{table}
\usepackage[utf8x,utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\urlstyle{same}

\begin{document}

\title{Istrazivanje Podataka}

\author{Daniel Do≈æa}

\maketitle

\section{Sta je istrazivanje podataka?}

Ne postoji precizna definicija istrazivanja podataka, ali se smatra da je to skup algoritama i
tehnika koji omogucava automatsko zakljucivanje nekih cinjenica i veza iz velikog skupa podataka,
koji bi mozda inace ostali neotkriveni.

\section{Koji su zadaci istrazivanja podataka?}

Zadaci u istrazivanju podataka se mogu podeliti u dve grupe: zadaci opisivanja i zadaci
predvidjanja. Zadaci predvidjanja bave se odredjivanjem vrednosti nekog atributa na osnovu vrednosti
neki drugih atributa. Ove atribute nazivamo \emph{zavisne promenljive} ili \emph{ciljne promenljive}
a atribute na osnovu kojih dolazimo do zakljucaka \emph{opisne promenljive} ili \emph{nezavisne
promenljive}.

\begin{enumerate}
    \item \textbf{Klasifikacija} pravimo funkciju od nezavisnih varijabli da bi dobili zavisne
        varijable
    \begin{itemize}
        \item klasifikacija - podatke, na primer: da li ce kupac kupiti neki artikal
        \item regresija - za kontinualne podatke, na primer: koliko ce stepeni biti u neko vreme
    \end{itemize}
    \item \textbf{Prepoznavanje obrazaca} - nalazimo veze u podacima: na primer ako je kupac kupio
        pelene, ver
    \item \textbf{Klasterovanje} - grupisemo podatke prema slicnosti, na primer: grupisemo dokumente
        vezane za ekonomiju u jedan klaster, a dokumente vezane za medicinu u drugi klaster.
    \item \textbf{Otkrivanje anomalija} - otkrivanje podataka koji su veoma drugaciji od ostalih.
        Ove podatke nazivamo anomalije ili autlajeri
\end{enumerate}

\section{Sta su podaci?}

Skup podataka mozemo predstaviti kao skup \emph{objekata}. Ovi objekti se takodje mogu nazivati i
\emph{slog}, \emph{slucaj}, \emph{uzorak}, \emph{vektor}\ldots Objekte opisuju njihovi
\emph{atributi} koji nam govore o osobinama tog objekta, na primer: boja, visina, tezina\ldots
Atributi se jos nazivaju i \emph{karakteristika}, \emph{varijabla}, \emph{polje}\ldots

\section{Sta je atribut?}

Atribut je karakteristika objekta. Atribut moze da varira izmedju razlicitih objekata ili da varira
u vremenu. Boja ociju je razlicita za dva coveka, a na primer, temperatura nekog predmeta varira u
toku vremena.

\section{Sta je vrednost atributa?}

Vrednost atributa je broj ili simbol koji je pridruzen atributu. Moramo razlikovati vrednost
atributa od samog atributa, jer vrednosti mogu da imaju neke osobine koje atribut nema, i obrnuto.
Na primer ako za zaposlenog cuvamo identifikacioni broj i broj godina, ima smisla racunati prosek
godina, dok nema smisla racunati prosek identifikacionog broja. Jedina logicna operacija sa
identifikacionim brojevima je poredjenje jednakosti.

\section{Kako odredjujemo tip atributa?}

Tip atributa mozemo odrediti na osnovu broja vrednosti koji moze da sadrzi:

\begin{itemize}
    \item \textbf{diskretni} (konacni, ili prebrojivo beskonacni skupovi vrednosti, primer:
        postanski brojevi)
    \item \textbf{neprekidni} (realni brojevi, primer: temperatura, tezina, pritisak, brzina)
\end{itemize}

\emph{Asimetricni atributi} - kod njih je bitna samo ne-nula vrednost. Na
primer vrednost 1 ako je student pohadjao kurs, a vrednost 0 ako nije. U tom slucaju bi nas zanimali
samo studenti sa vrednoscu 1.

Takodje, tip atributa mozemo posmatrati na osnovu operacija koje se mogu izvrsiti nad njihovim
vrednostima

\begin{center}\begin{tabular}{llll}
    \hline
    Vrsta Operacije & Rbr & Operacija & Tip Atributa \\ \hline
    razlicitost & 1 & \(=\) i \(\neq\)& Imenski(1) \\
    uredjenje & 2 & \(<\), \(>\), \(\leqslant\), \(\geqslant\) & Redni(1,2) \\
    aditivnost & 3 & \(+\), \(-\) & Intervalni(1,2,3) \\
    multiplikativnost & 4 & \(\times\), \(\div\) & Razmerni(1,2,3,4) \\ \hline
\end{tabular}\end{center} 

\begin{itemize}
    \item \textbf{Kategoricki} - imenski i redni
    \item \textbf{Numericki} - intervalni i razmerni
\end{itemize}

\section{Koje su karakteristike skupa podataka?}

\begin{enumerate}
    \item dimenzionalnost - predstavlja broj atributa koje objekti imaju
    \item retkost - uzmimo na primer asimetricne podatke, mozda razmatramo samo 1\% od ukupnog broja
        objekata
    \item rezolucija - sa kojim nivoom detalja gledamo na podatke. Na primer zemlja gledana sa par
        metara je proprilicno neravna, a gledano sa par desetina kilometara je poprilicno glatka.
\end{enumerate}

\section{Koji su tipovi skupa podataka?}

Tipovi podataka nisu precizno definisani, ali ih mozemo grupisati u tri grube kategorije:

\begin{enumerate}
    \item Slogovi
    \item Grafovski podaci
    \item Uredjeni podaci
\end{enumerate}

\section{Sta su slogovi?}

Podrazumeva da su podaci organizovani kao torke koje predstavljaju objekte. Svaki objekat ima fiksan
broj atributa. U svojoj osnovnoj formi, smatramo da ne postoje veze izmedju torki, kao ni izmedju
atributa, i svaki objekat ima isti skup atributa.

\paragraph{Transakcije}

Transakcija je specijalan slucaj torki. Asocijacija se moze napraviti sa \emph{potrosackom korpom}.
Svaka torka sadrzi skup elemenata koje je kupac kupio u jednoj kupovini.

\paragraph{Matrice podataka}

Ukoliko svi objekti imaju isti broj atributa, podaci se mogu posmatrati kao matrica. Redovi ove
matrice predstavljaju objekte, a kolone predstavljaju atribute (obrnuto je takodje dozvoljeno). U
ovakvom uredjenju, objekte mozemo posmatrati kao \(n\)-dimenizione vektore, gde su
dimenzije odredjene atributima.

\section{Sta su grafovski podaci?}

\paragraph{Grafovi sa vezama izmedju podataka u granama}

Objekti su u ovom modeli predstavljeni kao cvorovi grafa, dok su veze izmedju objekata prikazane
granama. Kao primer mozemo uzeti Web stranice.

\paragraph{Objekti predstavljeni kao grafovi}

Ako su objekti sacinjeni od podobjekata koji imaju medjusobne veze, takve objekte cesto
predstavljamo kao grafove. Kao primer mozemo uzeti hemijska jedinjenja, gde cvorovi predstavljaju
atome, a grane predstavljaju hemijske veze.

\section{Sa su uredjeni podaci?}

Ponekad su podaci uredjeni na osnovu vremena i/ili prostora.

\paragraph{Vremenski podaci}

Uredjeni su kao slogovi, s tim sto je svakom slogu pridruzeno vreme. Ovim mozemo odredjivati na
primer porast u kupovini slatkisa pred noc vestica.

\paragraph{Podaci u odredjenom redosledu}

Slicni su vremenskim podacima, ali nemaju vremenske odrednice, vec su poredjani u uredjenom
rasporedu. Primer su genetske sekvence.

\paragraph{Serijski podaci}

Vrlo slicni vremenskim podacima, samo sto svaki objekat cini serija podataka izmerenih u toku nekog
perioda.

\paragraph{Prostorni podaci}

Objekti sadrze prostorne odrednice. Primer bi bio podaci o vremenskoj prognozi.

\section{Sta je prepoznavanje obrazaca?}

Prepoznavanje obrazaca se moze, u osnovnoj formi predstaviti preko binarne matrice. Moze se
posmatrati takva matrica da kolone predstavljaju artikle, a redovi transakcije kupaca. Polje
\((i,j)\) je jednako 1 ako je kupac kupio artikal, inace je 0. Cilj nam je da prepoznamo da li postoje
neka pravila u kupovini, tojest, da li su vece sanse da kupac kupi stvar \(x\) ako je kupio stvar
\(y\). Formalno: neka je data binarna matrica \(n \times d\), posmatramo podskupove kolona, takve
da sve imaju vrednost 1. Svakom tom podskupu se dodeljuje podrska \(s\), koja predstavlja
ucestalost ponavljanja tog podskupa u odnosu na ceo skup. Ukoliko je \(s\) vece od minimalne
podrske, smatramo da je obrazac cest.

\section{Sta je podrska pravila pridruzivanja?}

Neka su \(A\) i \(B\) dva skupa obrazaca. Podrska \(sup(A \implies B)\) je
definfinisana kao \(\#(A \cup B) / N\), Gde \(\#(A \cup B)\) predstavlja broj
zadovoljavajucih obrazaca, a \(N\) ukupan broj redova u kompletnom skupu.

\section{Sta je pouzdanost pravila pridruzivanja?}

Neka su A i B dva skupa obrazaca. Pouzdanost \(conf(A \implies B)\) je definisana kao
\(\#(A \cup B) / \#(A)\).

\section{Sta je klasterovanje? (gruba definicija)}

Klasterovanje je grupisanje objekata na osnovu njihove `slicnosti'. Uvidja se da je od velike
vaznosti dizajn funkcije `slicnosti'.

\section{Sta je otkrivanje elemenata van granica? (gruba definicija)}

Zadatak je otkriti element koji je u velikoj meri razlicit od svih ostalih. Ovaj element zovemo
anomalija ili autlajer. Mozemo autlajere posmatrati i kao `element koji je toliko razlicit od
ostalih, da se moze posumnjati da je nastao nekim razlicitim mehanizmom'. Primeri bi bili upad u
racunarski sistem, zloupotreba kreditnih kartica\ldots

\section{Sta je klasifikacija? (gruba definicija)}

Klasifikacija je problem odredjivanja vrednosti nekog specijalnog atributa. Problem klasifikacije je
problem nadgledanog ucenja. Formiramo skup podataka koje nazivamo  podaci za trening, na osnovu
kojih nas algoritam odredjuje odnos ostalih atributa i specijalnog atributa koji se trazi. Nakon
toga test podaci se koriste da se utvrdi preciznost algoritma i eventualno podese parametri u cilju
povecanja preciznosti. Onda mozemo koristiti dobijen algoritam za odredjivanje specijalnog atributa
u skupovima podataka gde je on nepoznat.

\section{Navesti par primera istrazivanja podataka}

\begin{enumerate}
    \item rasporedjivanje artikala u prodavnici
    \item prepouke kupcima
    \item anomalije u logovima aplikacija
\end{enumerate}

\section{Sta je slicnost/razlicitost objekata, obrazaca i atributa?}

Slicnost i razlicitost izmedju objekata su funkcije koje nam govore o tome koliko su dva objekta
slicna ili razlicita. Vece vrednosti funkcije slicnosti nam govore da su objekti vise slicni, a
obrnuto vazi za funkcije razlicitosti.  Uglavnom se funkcije slicnosti mere u vrednostima na
intervalu [0, 1], dok se funkcije razlicitosti mere na intervalu od 0 (objekti su isti) na vise.
Koriste se i termini rastojanje (distance), blizina (proximity).

Funkcije slicnosti i razlicitosti su od velikog znacaja, jer imaju uticaj na svaki problem u
istrazivanju podataka. Los izbor funkcije slicnosti moze da ima presudnu vrednost u tome da li smo
odradili dobar posao. Ova cinjenica nam govori da ne smemo zapostaviti izbor funkcije slicnosti i
samo se fokusirati na algoritamski deo problema istrazivanja podataka.

\section{Navesti primer funkcije slicnosti/razlicitosti za nominalne atribute \(p\) i \(q\).}

\begin{itemize}
\item \textbf{Slicnost}

\[sim(p, q) = 1 \iff p = q\]
\[sim(p, q) = 0 \iff p \neq q\]

\item \textbf{Razlicitost}

\[dist(p, q) = 0 \iff p = q\]
\[dist(p, q) = 1 \iff p \neq q\]

\end{itemize}

\section{Navesti primer funkcije slicnosti/razlicitosti za redne atribute \(p\) i \(q\)}.

Ako \(p\) i \(q\) mogu imati \(n\) razlicitih vrednosti, onda funkcije slicnosti i
razlicitosti definisemo na sledeci nacin:

\begin{itemize}
\item \textbf{Slicnost}

\[sim(p, q) = 1 - \dfrac{|p - q|}{n - 1}\]

\item \textbf{Razlicitost}

\[dist(p, q) = \dfrac{|p - q|}{n - 1}\]
\end{itemize}

\section{Navesti primer funkcije slicnosti/razlicitosti za intervalne i razmerne atribute \(p\) i
\(q\).}

\begin{itemize}
\item \textbf{Slicnost}

\[sim(p, q) = -dist(p,q)\]
\[sim(p, q) = \dfrac{1}{1 + dist(p, q)}\]

\item \textbf{Razlicitost}

\[dist(p, q) = |p - q|\]
\end{itemize}

\section{Sta treba da vazi za funkciju rastojanja \(d\) da bi ona bila metrika?}

Da bi funkcija rastojanja \(d\) je metrika ako i samo ako vazi:

\begin{enumerate}
    \item Pozitivna odredjenost
        \[d(p, q) \geq 0, \forall p, q\]
        \[d(p, q) = 0, \iff p = q\]
    \item Simetrija
        \[d(p, q) = d(q, p)\]
    \item Nejednakost trougla
        \[d(p, q) \leq d(p, r) + d(r, q)\]
\end{enumerate}

\section{Sta treba da vazi za funkciju rastojanja \(d\) da bi ona bila ultrametrika?}

Funkcija \(d\) je \emph{ultrametrika} ako je metrika i ako vazi:

\[d(p, q) \leq max\{d(p, z), d(z, q)\}, \forall p, q, z\]

\section{Koje se mere rastojanja cesto koriste za kvantitativne podatke?}

Hamingovo rastojanje, rastojanje Minkovskog, Mahalanobisovo rastojanje.

\section{Sta je rastojanje Minkovskog (prednosti/nedostaci)?}

Za dva objekta \(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\)
Rastojanje minkovskog se definise kao:

\[\sum_{i=1}^{n} {({|x_i - y_i|}^p)}^{1/p}\]

Rastojanje Minkovskog za \(p = 2\) je Euklidsko rastojanje, za \(p = 1\) je Menhetn
rastojanje. Prednost ove metode je u tome sto je veoma intuitivna. Medjutim, to sto je intuitivna,
ne znaci da daje dobre rezultate, pogotovo u slucajevima velike dimenzionalnosti. Na primer ne uzima
u obzir koliko je neki atribut bitan za odredjivanje slicnosti. Takodje lose radi ako je nepoznata
raspodela\ldots

\section{Sta je Mahalanobisovo rastojanje (prednosti/nedostaci)?}

Jedan od nedostataka rastojanaj Minkovskog je sto zavisi samo od objekata nad kojim se formula
izracunava, a ne obraca paznju na distribuciju ostalih podataka. Mahalanobisovo rastojanje uzima u
obzir raspodelu podataka koristeci matricu kovarijansi. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) dva objekta.
Mahalanobisovo rastojanje izmedju \(\overline{X}\) i \(\overline{Y}\) je:

\[
    Maha(\overline{X}, \overline{Y}) =
    \sqrt{
        (\overline{X} - \overline{Y}) \times \Sigma^{-1} \times (\overline{X} - \overline{Y})^{T}
    }
\]

Drugim recima uzimamo razliku vektora \(\overline{X}\) i \(\overline{Y}\) pomnozimo sa inverzom
matrice kovarijansi \(\Sigma\) i transponovanom razlikom vektora \(\overline{X}\) i \(\overline{Y}\).

\section{Kako se moze definisati slicnost podataka sa kategorickim atributima?}

Kad radimo sa kategorickim funkcijama obicno se vise koriste funkcije slicnosti nego razlicitosti
jer je se diskretne vrednosti mogu prirodnije porediti.  Slicnost podataka sa kategorickim
atributima se moze definisati preko slicnosti njihovih pojedinacnih atributa. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti. Njihovu
slicnost mozemo definisati kao:

\[
    Sim(\overline{X}, \overline{Y}) = \sum_{i = 1}^{n} S(x_i, y_i)
\]


Odavde vidimo da izbor funkcije \(S\) odredjuje citavu funkciju slicnosti. U najjednostavnijem
slucaju funkcija \(S\) se moze definisati kao

\[
    S(x_i, y_i) =
    \begin{cases}
        1 & x_i = y_i \\
        0 & x_i \neq y_i \\
    \end{cases}
\]

Medjutim, mozemo uvideti da je problem kod ove funkcije da ona ne uzima u obzir frekvenciju
razlicitih atributa. Uzmimo na primer atribut koji moze da ima vrednosti `Normalno', `Rak' i
`Dijabetes'. Najverovatnije je da ce 99\% podataka imati vrednost `Normalno' ali oni nece biti od
statisticke vaznosti isto toliko koliko i objekti sa vrednostima `Rak' i `Dijabetes'. Drugim recima,
velika vecina nam ne odredjuje dovoljno dobro slicnost izmedju objekata. Sa ovim na umu treba
kreirati nesto slicno Mahalanobisovom pristupu. Takav pristup naziva se \emph{inverzna frekvencija
ponavljanja}.

 Neka je \(p_i(x)\) broj slogova ciji \(i\)-ti atribut ima vrednost \(x\). Tada mozemo nasu funkciju
 \(S\) definisati kao:

 \[
    S(x_i, y_i) =
    \begin{cases}
        \dfrac{1}{p_i(x_i)^2} & x_i = y_i \\
        0 & x_i \neq y_i \\
    \end{cases}
 \]

\section{Kako se odredjuje slicnost tekstualnih dokumenata?}
\label{pitanje:slicnost_tekst_dokumenata}

Tekstualne dokumente mozemo smatrati multidimenzionim podacima kada bi ih posmatrali kao `vrece
reci'. To bi znacilo da bi kompletan set atributa nekog dokumenta bio ceo leksikon reci, a vrednosti
bi bile broj pojavljivanja odgovarajuce reci u dokumentu. Ovakav format bi znacio da ce vecina
atributa imati vrednost 0, sto bi dalje povlacilo da kada bismo koristili nesto kao sto je
rastojanje Minkovskog, dva slicna dugacka teksta ce uvek biti vise razlicita nego dva zapravo
razlicita kraca teksta. Da bismo ovo izbegli, koristimo kosinusno rastojanje. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti. Kosinusno
rastojanje definisemo kao:

\[
    \cos(\overline{X}, \overline{Y}) =
    \dfrac{\sum_{i=1}^{n} x_i \cdot y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}
\]

Ova metoda ne uzima u obzir odnos pojavljivanja reci. Mi znamo da su tekstovi slicniji ukoliko se
podudaraju na recima koje se retko javljaju, nego na onim koje se cesto javljaju. \emph{Inverznu
frekvenciju dokumenta} definisemo kao:
\[id_i = \log(n/n_i)\]
gde je \(n\) ukupan broj dokumenata, a \(n_i\) je broj dokumenata u kojima se \(i\)-ta rec
pojavljuje.

Kako pojavljivanje jedne reci ne bi trebalo da poremeti celu meru, koristimo funkcije:

\[f(x_i) = \sqrt{x_i}\]
\[f(x_i) = \log(x_i)\]

\emph{Normalizovane} funkcije frekvencije reci se onda definisu kao

\[h(x_i) = f(x_i) \cdot id_i\]

Sada mozemo napisati kosinusnu meru slicnosti koristeci ove normalizovane funkcije frekvencije
ponavljanja reci

\[
    \cos(\overline{X}, \overline{Y}) =
    \dfrac{\sum_{i=1}^{n} h(x_i) \cdot h(y_i)}{\sqrt{\sum_{i=1}^{n} h(x_i)^2} \cdot
        \sqrt{\sum_{i=1}^{n} h(y_i)^2}}
\]


\section{Sta je rastojanje Minkovskog sa tezinama?}

U nekim slucajevima, nisu svi atributi objekta podjednako bitni u odredjivanju slicnosti. Na primer,
visina plate igra mnogo vecu ulogu nego pol u slucaju odobravanja kredita. U ovakvom slucaju mozemo
koristiti rastojanje Minkovskog sa tezinama (generalizovano rastojanje Minkovskog).

\[
    Sim(\overline{X}, \overline{Y}) = \left(\sum_{i=1}^{n} a_i \cdot |x_i - y_i|^p\right)^{1/p}
\]

Vrednost \(a_i\) nam govori o vaznosti \(i\)-tog atributa u poredjenju dva
objekta. Vrednosti \(a_i\) se dobijaju heuristickim metodama i u velikoj meri zavise od iskustva
analiticara.

\section{Kako se odredjuje slicnost dva sloga sa kvantitativnim i kategorickim atributima?}

Poprilicno jednostavan pristup resavanju ovog problema je da dodamo tezine dobijene za numericke
parametre i tezine dobijene za kategoricke parametre. Neka su
\(\overline{X}=(\overline{X_n}, \overline{X_c})\) i \(\overline{Y}=(\overline{Y_n}, \overline{Y_c})\)
objekti koji sadrze numericke i kategoricke atribute.

\[
    Sim(\overline{X}, \overline{Y}) =
        \lambda \cdot NumSim(\overline{X_n}, \overline{Y_n})
        + (1 - \lambda) \cdot CatSim(\overline{X_c}, \overline{Y_c})
\]

Ovde \(\lambda\) oznacava bitnost kategorickog i numerickog dela u izracunavanju slicnosti. Izbor
\(\lambda\) je tezak, pogotovo sa ogranicenim poznavanjem domena podataka. Generalno se uzima udeo
numerckih atributa u ukupnom skupu atributa, mada ovo ne znaci da je to dobar izbor.

\section{Sta je SMC (simple matching coefficient)?}
SMC (jednostavno uparivanje koeficijenata) se koristi za objekte sa binarnim atributima. Neka su
\(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti sa binarnim
atributima. Definisemo
\begin{itemize}
    \item \(M_{01}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{10}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 0 u \(\overline{Y}\)
    \item \(M_{11}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{00}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 0 u \(\overline{Y}\)
\end{itemize}
Tada je
\[SMC(\overline{X}, \overline{Y}) = \dfrac{M_{11} + M_{00}}{M_{00} + M_{11} + M_{10} + M_{01}} \]

\section{Sta su Zakardovi koeficijenti? Kada se koriste?}
Zakardovi koeficijenti se koriste u slucaju objekata sa asimetricnim binarnim atributima. Definise
se kao kolicnik broja parova gde su obe vrednosti ne-nula i broja parova gde nisu obe vrednosti
nula. Neka su \(\overline{X}=(x_1,x_2,\ldots,x_n)\) i \(\overline{Y}=(y_1,y_2,\ldots,y_n)\) objekti
sa asimetricnim binarnim atributima. Definisemo
\begin{itemize}
    \item \(M_{01}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{10}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 0 u \(\overline{Y}\)
    \item \(M_{11}\) --- broj atributa koji su jednaki 1 u \(\overline{X}\) i 1 u \(\overline{Y}\)
    \item \(M_{00}\) --- broj atributa koji su jednaki 0 u \(\overline{X}\) i 0 u \(\overline{Y}\)
\end{itemize}
Tada zakardove koeficijente izracunavamo
\[Jacc(\overline{X}, \overline{Y}) = \dfrac{M_{11}}{M_{11} + M_{10} + M_{01}}\]

\section{Sta su prosireni Zakardovi koeficijenti (koeficijenti Tanimoto-a)?}
\emph{Prosireni Zakardovi koeficijenti} se koriste u slucaju kada atributi nisu binarni, ali se
svodi na slucaj \emph{Zakardovih koeficijenata} ukoliko jesu.
\[
    ExtJacc(\overline{X},\overline{Y}) =
    \dfrac{
        \overline{X} \bullet \overline{Y}
    }{
        ||\overline{Y}||^2 + ||\overline{Y}||^2
        - \overline{X} \bullet \overline{Y}
    }
\]

\section{Kako se definise kosinusna slicnost dva objekta? Kada se koristi?}

Pitanje~[\ref{pitanje:slicnost_tekst_dokumenata}]

\section{Sta je korelacija dva objekta?}
Korelacija izmedju dva objekta koji imaju binarne ili neprekidne atribute je mera linearne
zavisnosti izmedju tih atributa.
\[
    \rho_{xy} = \dfrac{cov_{xy}}{\sigma_x \cdot \sigma_y}
\]
Kovarijansa
\[
    cov_{xy} = \dfrac{1}{n-1}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})
\]
Standardna devijacija
\[
    \sigma_x = \sqrt{\dfrac{1}{n-1}\sum_{i=1}^{n} (x_i - \overline{x})^2}
\]
Srednja vrednost
\[
    \overline{x} = \dfrac{1}{n}\sum_{i=1}^{n} x_i
\]

Korelacija je uvek u intervalu \([-1,1]\). Korelaciju vrednosti 1 zovemo perfektna pozitivna
korelacija. Slicno je za negativnu perfektnu korelaciju.


\section{Kako izrazavamo slicnost diskretnih podataka?}
Slicnost diskretnih podataka mozemo predstaviti kao cenu transformacije jednog objekta u drugi.
Za prvih \(i\) atributa iz \(\overline{X}=(x_1,x_2,\ldots,x_n)\) i prvih \(j\) atributa iz
\(\overline{Y}=(y_1,y_2,\ldots,y_n)\), edit rastojanje je:

\[
    Edit(i, j) = min
    \begin{cases}
        Edit(i-1, j) + cena\_brisanja \\
        Edit(i, j-1) + cena\_umetanja \\
        Edit(i-i, j-1) + I_{ij} \cdot cena\_zamene \\
    \end{cases}
\]
Vrednost \(I_{ij}\) je 1 ukoliko su \(i\)-ti simbol u \(\overline{X}\) i \(j\)-ti simbol u
\(\overline{Y}\) isti, inace je 0.

Jos jedan pristup koji se koristi za poredjenje objekata sa diskretnim atributima, je metoda
najduzeg zajednickog podniza, ali takvog da ne moraju elementi biti jedan za drugim.
\[
    LCSS(i, j) =
    \begin{cases}
        \emptyset & i = 0 \lor j = 0 \\
        LCSS(i-1, j-1) & i,j > 0 \land x_i = y_i \\
        longest\{LCSS(i, j-1), LCSS(i-1, j)\} & i,j > 0 \land x_i \neq y_i \\
    \end{cases}
\]

\section{Sta je entropija?}
Entropija dogadjaja \(X\) je
\[
    H(X) = - \sum_{i=1}^{n-1}p_i\log_{2}p_i
\]
Broj \(n\) obelezava broj razlicitih klasa podataka koje razmatramo, a \(p_i\) je broj slogova
koji pripadaju klasi \(i\).

\section{Sta su mere na osnovu gustina? Koje se metode najcesce koriste?}
Mere na osnovu gustina odredjuju stepen bliskosti objekata u nekoj oblasti. Najcesce se koristi pri
klasterovanju, i otkrivanju anomalija. Najcesce se koriste
\begin{itemize}
        \item Euklidska gustina --- broj tacaka po jedinici povrsine/zapremine
        \item Gustina verovatnoca --- procena distribucije podataka na osnovu izgleda
        \item Graf zasnovane gustine --- na osnovu povezanosti
\end{itemize}
Primer euklidske gustine bi bila podela regiona na neki broj celioja i definisanje gustine preko
broja tacaka u celijama.

Jos jedan primer bi mogao biti udaljenost od neke centralne tacke.

\section{Sta je priprema podataka? Zasto se vrsi?}
Ravni podaci su uglavnom veoma razliciti jedni od drugih. Podaci mogu da fale, da budu
nekonzistentni, pogresni itd\ldots Cilj je podatke dovesti u takvo stanje da se na njih mogu
primeniti razliciti pristupi istrazivanja podataka. Drugim recima, cilj name je da izvucemo sve
korisne informacije iz ovih izvora podataka. Neki od pristupa koji se koriste su
\begin{itemize}
    \item Izdvajanje karakteristika
    \item Ciscenje podataka
    \item Izbor i transformacija podataka
    \item Redukcija podataka
\end{itemize}

\section{Sta je izdvajanje karakteristika?}
Ravni podaci su cesto u formi koji nisu dobri za procesiranje. Uzmimo za primer logove aplikacije,
dokumente itd\ldots Cilj nam je da iz ovakvih nestruktuiranih podataka, izvucemo osobine sa kojima
mozemo da radimo, i koje ce nam pomoci u resavanju konkretnog zadatka istrazivanja podataka. Ovo
se moze smatrati i najbitnijim delom celog procesa istrazivanja podataka, jer analiza podataka moze
biti samo onoliko dobra koliko su i podaci dobri.


\section{Sta je prenosivost tipova podataka?}
Osobina podataka uglavnom jeste da su heterogeni. To znaci da objekti imaju atribute razlicitih
tipova. Ovo umnogome ogranicava izbor postojeceg algoritma za obradu podataka. Zbog ovoga postoji
koncept prenosivosti tipova. Naravno, pretvaranjem jednog tipa u drugi, moguce je izgubiti neke
informacije o podacima. Zbog ovoga treba biti pazljiv prilikom primene metoda za prevodjenje tipova.
Neke od popularnih metoda su

\begin{itemize}
    \item Diskretizacija
    \item Binarizacija
    \item Latentna Semnticka Analiza (LSA)
    \item Simbolicka aproksimacija agregata (SAX)
    \item Diskretna Furijeova Transformacija (DFT)
    \item Diskretna Transformacija Talasicima (DWT)
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Tablela prevodjenja tipova}
    \begin{tabular}{||c|c|c||}
        \hline
        \textbf{Izvorni tip} & \textbf{Ciljni tip} & \textbf{Metode} \\ \hline \hline
        Numericki & Kategoricki & Diskretizacija \\ \hline
        Kategoricki & Numericki & Binarizacija \\ \hline 
        Tekst & Numericki & LSA \\ \hline
        Vremenski intervali & Diskretne niske & SAX \\ \hline
        Diskretne niske & Numericki multidimenzioni & DWT, DFT \\ \hline
        Vremenski intervali & Numericki multidimenzioni & DWT, DFT \\ \hline \hline
    \end{tabular}
\end{table} 

\section{Sta je diskretizacija? Kada se koristi? Kako se biraju intervali?}
Najcesce transformisemo podatke iz numerickih u kategoricke. Ovaj proces nazivamo
\emph{diskretizacija}. Osnovni pristup je da numericke podatke podelimo u intervale, svakom
intervalu pridruzimo jednu diskretnu vrednost i onda atribute sa numerickom vrednoscu iz nekog
intervala prevedemo u odgovarajucu kategoricku vrednost za taj interval. Na primer godine podelimo
na \([1, 10]\), \([11, 20]\), \([21, 30]\) i intervale preslikamo u diskretne vrednosti, na primer,
\(A\), \(B\), \(C\) onda ce simbolicka vrednost svakog objekta sa godinama u intervalu \([1, 10]\)
biti \(A\)\ldots Posto se podaci u jednom intervalu ne razlikuju, diskretizacijom gubimo informacije
ali se ispostavlja da ovaj gubitak nije od presudnog znacaja za analizu podataka. Nabrojacemo
nekoliko nacina biranja intervala.
\begin{enumerate}
    \item \emph{Jednake sirine intervala} --- Interval podelimo na \(n\) delova jednake duzine.
        Svaki interval \([a, b]\) se bira tako da je \(b - a\) jednako za svaki interval. Ovaj
        pristup nije dobar, ukoliko raspodela vrednosti atributa nije uniformna.
    \item \emph{Jednaki logaritmi sirine intervala} --- Svaki interval \([a, b]\) se deli na nacin
        da je \(\log(b) - \log(a)\) jednako za svaki interval. Ovaj pristup ima efekat da se
        intervali geometrijski sire \([a, a \cdot \lambda]\),
        \([a\cdot \lambda, a \cdot \lambda^2]\), i tako dalje za \(\lambda > 1\)
    \item \label{podela_intervala:jednak_br_el} \emph{Jednak broj elemenata u intervalu} --- Ideja
        je da se intervali odrede tako da svaki interval sadrzi jednak broj objekata. Ovo se moze
        postici sortiranjem vrednosti atributa, i onda odredjivanjem tacaka podele nad sortiranim
        nizom.
\end{enumerate}

\section{Sta je binarizacija? Kada se koristi?}
\emph{}Binarizacija je postupak transformacije kategorickih podataka u numericke radi lakse primene
nekih algoritama. Ako kategoricki atribut ima \(n\) razlicitih vrednosti, onda kreiramo \(n\)
binarnih atributa. Svaki binarni atribut odgovara jednom kategorickom. Dakle, samo jedan binarni
atribut ce imati vrednost 1. Binarizacija se obicno primenjuje na atribute u analizi zasnovanoj na
pravilima
pridruzivanja.

\section{Kako se tekstualni podaci prevode u numericke?}
Iako vektorska reprezentacija teksta moze da se posmatra kao redak vektor ogromne dimenzionalnosti,
ovaj pristup nije najbolji. Uglavnom za ovakav format je bolje koristiti, na primer, kosinusnu
slicnost, nego Euklidsko rastojanje. U svakom slucaju, bilo bi dobro kada bismo mogli da koristimo
algortme koji rade sa numerickim podacima.

Prvi korak u transformisanju je koriscenje \emph{latentne semanticke analize (LSA)} kako bismo tekst
pretvorili u vektor koji nije redak i ima nizu dimenzionalnost. Nakon toga, svaki dokument
\(\overline{X} = (x_1, \ldots, x_n)\) mora biti skaliran na
\(\dfrac{1}{\sqrt{\sum_{i=1}^{n}x_i^2}}(x_1, \ldots, x_n)\). Ovo skaliranje je neophodno da bi
osiguralo da se dokumenti razlicitih duzina tretiraju na uniforman nacin. Nakon ovog skaliranja,
mere poput Euklidskog rastojanja su efektivnije.

\section{Kako se podaci iz vremenskih serija prevode u diskretne niske?}
Vremenske serije se mogu prevesti u diskretne niske pomocu \emph{simbolicke aproksimacije agregata
(SAX)}. Ova metoda se sastoji iz dva koraka
\begin{enumerate}
    \item Podelimo vremensku seriju na \(x\) jednakih delova, pa onda za svaki deo izracunamo
        prosecno vreme.
    \item U ovom koraku diskretizujemo vrednosti pomocu podele
        intervala~\ref{podela_intervala:jednak_br_el}. Umesto da sortiramo elemente vremenske serije
        pretpostavljamo da vrednosti u vremenskim serjiama imaju normalnu (Gausovu) raspodelu.
        Koristeci standardnu devijaciju i srednju vrednost, odredjujemo parametre raspodele. Kao
        granice intervala za diskretizaciju uzimamo kvantile ove raspodele (uglavnom 3 do 10). Ovim
        smo napravili simbolicku reprezentaciju vremenske serije, koja je sad zapravo diskretna
        niska.
\end{enumerate}

\section{Kako se diskretne niske prevode u numericke podatke?}
Transformacija se vrsi u dva koraka
\begin{enumerate}
    \item Za svaki simbol iz diskretne sekvence se kreira jedan red binarnih vrednosti, duzine
        originalne sekvence, gde broj 1 nalazi na svakom mestu na kome se taj simbol nalazi u
        originalnoj sekvenci. Primer:
        \begin{table}[H]
            \centering
            \begin{tabular}{ccccc}
                \hline
                A & B & B & A & C \\ \hline
                1 & 0 & 0 & 1 & 0 \\
                0 & 1 & 1 & 0 & 0 \\
                0 & 0 & 0 & 0 & 1 \\
            \end{tabular}
        \end{table} 
    \item U ovom koraku se na dobijenu vrednost primenjuje transformacija talasicima, i na kraju se
        osobine iz razlicitih serija spajaju kako bi se dobio jedan multidimenzioni slog.
\end{enumerate}

\section{Koji su aspekti ciscenja podataka?}
Razni faktori mogu da uticu na to da imamo nepotpune ili nekorektne podatke. Na primer, ljudi ne
zele da daju podatke iz privatnih razloga, ako su u pitanju senzori, moze se desiti da se desi
problem u prenosu podataka, mogu biti u pitanju ljudske greske pri rucnom unosu podataka\ldots Zbog
ovoga je ciscenje podataka neophodno. Aspekti ciscenja podataka su:
\begin{enumerate}
    \item Rad sa nedostajucim podacima
    \item Rad sa nekorektnim podacima
    \item Rad sa dupliranim podacima
    \item Skaliranje i normalizacija
\end{enumerate}

\section{Sta podrazumeva rad sa nedostajucim podacima?}
Nedostajuci podaci su cesti u bazama podataka gde metod sakupljanja podataka nije savrsen, Na primer
cesto se desava da na anketama ne budu odgovorena sva pitanja. Tri skupa tehnika se koristi kako bi
se prevazisao problem nedostajucih podataka:
\begin{enumerate}
    \item Ako nekom objektu nedostaje podataak, brisemo ga. Ovaj pristup nije toliko dobar ako su
        objekti sa nedostajucim vrednostima cesti
    \item Nedostajuce vrednosti se mogu pretpostaviti \emph{(imputacija)}. Imputacija, doduse, moze
        da utice na rezultate istrazivanja podataka.
    \item Algoritmi se modifikuju tako da mogu da podnesu nedostajuce vrednosti.
\end{enumerate}
Problem predvidjanja nedostajucih vrednosti je direktno vezan sa problemom klasifikacije, s tim sto
problem klasifikacije se bavi odredjivanjem jednog specijalnog atributa na osnovu ostalih, dok u
ovom slucaju vise vrednosti moze nedostajati, pa je time problem slozeniji.

\section{Sta podrazumeva rad sa nekorektnim podacima?}
Kljucne metode za otklanjane ili korekciju nekorektnih vrednosti su:
\begin{enumerate}
    \item \emph{Detekcija nekonzistentnosti} --- uglavnom je greska vidljiva u koliko imamo vise
        izvora podataka gde su podaci koji se odnose na istu vrednost razliciti.
    \item \emph{Domensko znanje} --- na primer, mozemo da znamo da ako polje vezano za drzavu ima
        vrednost `Srbija', polje glavni grad ne moze biti `London'.
    \item \emph{Metode orijentisane podacima} --- statisticko ponasanje podataka se moze iskoristi u
        pronalazenju autlajera. Treba obratiti paznju da ne mora uvek da znaci da su ove vrednosti
        anomalije, te se moraju rucno ispitati pa tek onda potencijalno odbaciti.
\end{enumerate}


\section{Sta podrazumeva rad sa dupliranim podacima?}
\section{Kada se i kako primenjuju skaliranje i normalizacija?}
I mnogo slucajeva, razliciti osobine su razlicito skalirane. Na primer broj godina i visina plate.
Visina plate je mnogo veca od broja godina, sto bi znacilo da bi rezultati primene bilo kog metoda
agregiranja podataka bili odredjeni vecinom od strane atributa sa vecim vrednostima.

Da bismo resili ovaj problem primenjujemo \emph{standardizaciju}. Neka \(j\)-ti atribut ima srednju
vrednost \(\mu_j\) i standardnu devijaciju \(\sigma_j\) i neka je \(\overline{X_i}\) \(i\)-ti slog.
Atribut \(x_{ij}\) mozemo skalirati kao:
\[
    z_{ij} = \dfrac{x_{ij} - \mu_j}{\sigma_j}
\]

Drugi pristup je da koristimo \emph{min, maks} skaliranje, kako bismo sveli atribute na vrednosti u
intervalu \([0, 1]\). Neka je \(\overline{X_i}\) \(i\)-ti slog i neka su \(min_j\) i \(max_j\) redom
minimalna i maksimalna \(j\)-og atributa. Atribut \(x_{ij}\) mozemo skalirati kao:
\[
    z_{ij} = \dfrac{x_{ij} - \min_j}{max_j - min_j}
\]
Ovaj pristup nije efektivan ukoliko su minimum i maksimum neki ekstremni autlajeri. Zamislimo
situaciju gde se greskom na broj godina dodala jos jedna nula, pa imamo maksimalni broj godina 800.
Ovo bi znacilo da ce se svi podaci nalaziti u intervalu \([0, 0.1]\)


\section{Zasto se podaci redukuju i transformisu? Nabrojati pristupe.}
Cilj smanjenja podataka je da se oni predstave kompaktnije. Kada je velicina podataka manja mnogo je
lakse primeniti algoritme na njih, pogotovo oni vece slozenosti. Redukcija podataka se moze vrsiti
na nivou slogova kao i na nivou atributa. Neke od metoda redukcije podataka su:
\begin{enumerate}
    \item Agregacija
    \item Uzimanje uzoraka
    \item Izbor karakteristika
    \item Redukcija podataka pomocu rotacije osa
    \item \ldots
\end{enumerate}

\section{Sta je agregacija? Koja je svrha agregacije?}
Agregacija je kombinovanje dva ili vise atributa (ili objekata) u jedan atribut (objekat). Svrha
agregacije je redukcija podataka (smanjivanje broja atributa ili objekata), promena skale,
stabilniji podaci (podaci koji su agregirani imaju tendenciju da imaju manja odstupanja)

\section{Sta je uzimanje uzoraka? Koji su tipovi uzoraka?}
Uzimanje uzoraka je `filtriranje' originalnih slogova u cilju kreiranja manje baze podataka.
Statisticari biraju uzorke jer je dobijanje kompletnog skupa podataka koji su od interesa jako skupo
i vremenski zahtevno. Izbor uzoraka se koristi kada je obrada kompletnog skupa podataka jako skupa
i/ili vremenski zahtevna.  Glavna prednost uzimanja uzoraka je sto je jednostavno, intuitivno i
relativno lako se implementira.

Kljucni principi za efektivan izbor uzoraka je da koriscenjem uzoraka treba da se dobije efekat
skoro isti kao da je radjeno na kompletnom skupu podataka. Uzorak je reprezentativan ako ima
proksimativno iste osobine kao i originalni skup podataka.

Generalno, razlikujemo dva razlicita slucaja pri uzimanju uzorka.
\begin{enumerate}
    \item Uzimanje uzoraka nad statickim podacima
    \item Uzimanje uzoraka nad tokom podataka
\end{enumerate}

\paragraph{Uzimanje uzoraka nad statickim podacima} se moze izvrsiti na vise nacina:
\begin{enumerate}
    \item \emph{Jednostavan slucajni uzorak } --- uzimamo elemente slucajnim izborom iz originalnog skupa
        podataka. Mozemo svaki element birati sa nekom sansom i time spreciti pojavu duplikata (osim
        u slucaju kada originalni skup podataka sadrzi duplikate), a mozemo svaki put birati iz
        celog skupa podataka i time ostaviti mogucnost pojave duplikata
    \item \emph{Pristrasno uzimanje uzoraka} --- neki podaci su vazniji od drugih, na primer podaci
        koji su skorije skupljeni imaju vece sanse da budu ukljuceni u uzorak.
    \item \emph{Stratifikovano uzimanje uzorka} --- podaci se dele na slojeve i iz svakog sloja se
        bira jednostavan slucajni uzorak. Ovaj pristup se koristi u situacijama kao sto su na primer
        kada pokusavamo da izmerimo ekonomske razlike zivota razlicitih pojedinaca neke populacije.
        Cak ni uzorak od milion ljudi mozda nece obuhvatiti nekog milionera. Zato se podaci
        raslojavaju po zaradi, i onda se svaki sloj nezavisno obradjuje.
\end{enumerate}

\paragraph{Uzimanje uzoraka nad tokom podataka} nekada podaci nad kojima moramo da uzmemo uzorak
nisu poznati u celosti jer su ogromnih razmera i ne mogu da stanu na disk. Dakle, moramo koristiti
neku metodu koja ce dinamicki i efikasno \emph{odrzavati} uzorak. To se svodi na dve odluke
\begin{enumerate}
    \item Kako odluciti da li prispeli objekat iz toka staviti u uzorak.
    \item Kako odluciti koji element iz trenutnog uzorka izbaciti kako bismo `napravili mesta' za
        novi objekat.
\end{enumerate}

\section{Sta je izbor karakteristika?}
Neke atribute mozemo jednostavno odbaciti zato sto nisu od vaznosti. Koji to atributi nisu od
vaznosti? To ocigledno zavisi od toga sta zelimo da postignemo analizom. Izbor karakteristika
takodje smanjuje broj redudantnih karakteristika (na primer broj poena i ocena) kao i dimenzionalnost
podataka. Veliki broj tehnika postoji, pogotovu za klasifikaciju.

Atributi ne moraju samo da se eliminisu. Cesto se formiraju novi atributi koji ukljucuju vazne
karakteristike zbog efikasnije obrade.

\section{Kako se podaci redukuju pomocu rotacije osa?}
TBD

\section{Sta je Principal Component Analysis (PCA)?}
TBD

\section{Sta je Singular Value Decomposition?}
TBD

\section{Navesti jos neke metode dimenzione redukcije?}
TBD

\end{document}
